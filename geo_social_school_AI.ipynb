{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea72a0d",
   "metadata": {},
   "source": [
    "# Calibrated geo-social link prediction for household–school connectivity in community resilience\n",
    "\n",
    "[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)\n",
    "[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-red.svg)](https://pytorch.org/)\n",
    "[![PyG](https://img.shields.io/badge/PyG-2.3+-green.svg)](https://pyg.org/)\n",
    "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Paper Reference\n",
    "\n",
    "This repository provides the **official implementation** for:\n",
    "\n",
    "> **\"Calibrated geo-social link prediction for household–school connectivity in community resilience\"**  \n",
    "> *International Journal of Disaster Risk Reduction*, Volume 131, December 2025, 105872  \n",
    "> DOI: [https://doi.org/10.1016/j.ijdrr.2025.105872](https://doi.org/10.1016/j.ijdrr.2025.105872)\n",
    "\n",
    "If you use this code in your research, please cite:\n",
    "\n",
    "```bibtex\n",
    "@article{gupta2025calibrated,\n",
    "  title={Calibrated geo-social link prediction for household–school connectivity in community resilience},\n",
    "  author={Gupta, Himadri Sen and Biswas, Saptadeep and Nicholson, Charles D.},\n",
    "  journal={International Journal of Disaster Risk Reduction},\n",
    "  volume={131},\n",
    "  pages={105872},\n",
    "  year={2025},\n",
    "  publisher={Elsevier},\n",
    "  doi={10.1016/j.ijdrr.2025.105872}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a **Heterogeneous Graph Neural Network (HGNN)** framework for predicting household-to-school attendance links using geo-social network data from synthetic populations. The model is designed to support disaster impact assessment and emergency planning by accurately modeling school enrollment patterns.\n",
    "\n",
    "### Key Contributions\n",
    "\n",
    "1. **Heterogeneous Graph Representation**: Jointly models households, schools, and multiple relationship types (attendance, employment, spatial proximity)\n",
    "2. **Hybrid Architecture**: Combines Heterogeneous Graph Transformer (HGT) with LightGCN via learned fusion gates\n",
    "3. **Self-Supervised Pre-training**: Contrastive learning (InfoNCE) with denoising reconstruction for robust representations\n",
    "4. **Calibrated Predictions**: Temperature scaling for well-calibrated probabilities suitable for decision support\n",
    "5. **Comprehensive Evaluation**: Standard split, proximity-controlled split, and cold-start scenarios (households/schools)\n",
    "\n",
    "---\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Feature Encoder** | MLP with multi-scale Fourier positional encoding for geographic coordinates and categorical embeddings for demographics |\n",
    "| **HGT Backbone** | Heterogeneous Graph Transformer with relation-aware message passing across node and edge types |\n",
    "| **LightGCN Branch** | Collaborative propagation on the bipartite household-school attendance graph |\n",
    "| **Fusion Gate** | Learnable gating mechanism to adaptively combine HGT and LightGCN representations |\n",
    "| **Calibrated Decoder** | MLP scoring function with temperature scaling for probability calibration |\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Software Dependencies\n",
    "\n",
    "```bash\n",
    "# Core dependencies\n",
    "python>=3.8\n",
    "torch>=2.0.0\n",
    "torch-geometric>=2.3.0\n",
    "torch-cluster>=1.6.0  # Required for KNN graph construction\n",
    "\n",
    "# Data processing\n",
    "pandas>=1.5.0\n",
    "numpy>=1.21.0\n",
    "\n",
    "# Evaluation\n",
    "scikit-learn>=1.0.0\n",
    "\n",
    "# Optional (recommended)\n",
    "psutil>=5.9.0  # For RAM monitoring\n",
    "matplotlib>=3.5.0  # For figure generation\n",
    "```\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Clone the repository\n",
    "git clone https://github.com/himadri-gupta/geo-social-school-gnn.git\n",
    "cd geo-social-school-gnn\n",
    "\n",
    "# Create conda environment (recommended)\n",
    "conda create -n geosocial python=3.10\n",
    "conda activate geosocial\n",
    "\n",
    "# Install PyTorch (adjust for your CUDA version)\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install PyTorch Geometric\n",
    "pip install torch-geometric\n",
    "pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "\n",
    "# Install remaining dependencies\n",
    "pip install pandas numpy scikit-learn psutil matplotlib\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Data Requirements\n",
    "\n",
    "The pipeline expects three CSV files from synthetic population data:\n",
    "\n",
    "| File | Description | Required Columns |\n",
    "|------|-------------|------------------|\n",
    "| `hui_*.csv` | Household Unit Information | `huid`, `ownershp`, `race`, `hispan`, `randincome`, `numprec` |\n",
    "| `prec_*_students.csv` | Student enrollment records | `huid`, `NCESSCH`, `SCHNAM09`, `hcb_lat`, `hcb_lon`, `ncs_lat`, `ncs_lon` |\n",
    "| `prec_*_schoolstaff.csv` | Staff employment records | `huid`, `SIName`, `hcb_lat`, `hcb_lon` |\n",
    "\n",
    "**Note**: Data is from the Housing Unit Inventory (HUI) and Person-Record (PREC) datasets for Lumberton, NC 2010 census. See [DesignSafe-CI Project PRJ-2961](https://www.designsafe-ci.org/).\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Quick Start (Jupyter Notebook)\n",
    "\n",
    "```python\n",
    "# Set file paths\n",
    "paths = {\n",
    "    'households': 'hui_v0-1-0_Lumberton_NC_2010_rs9876.csv',\n",
    "    'students': 'prec_v0-2-0_Lumberton_NC_2010_rs9876_students.csv',\n",
    "    'staff': 'prec_v0-2-0_Lumberton_NC_2010_rs9876_schoolstaff.csv'\n",
    "}\n",
    "\n",
    "# Run full robustness suite\n",
    "run_robustness_suite(paths, device='cuda', seed=42)\n",
    "```\n",
    "\n",
    "### Command Line Interface\n",
    "\n",
    "```bash\n",
    "python geo_social_school_AI.py \\\n",
    "    --mode robust \\\n",
    "    --households hui_v0-1-0_Lumberton_NC_2010_rs9876.csv \\\n",
    "    --students prec_v0-2-0_Lumberton_NC_2010_rs9876_students.csv \\\n",
    "    --staff prec_v0-2-0_Lumberton_NC_2010_rs9876_schoolstaff.csv \\\n",
    "    --seed 42 \\\n",
    "    --device cuda\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Experiments\n",
    "\n",
    "The evaluation protocol distinguishes multiple scenarios:\n",
    "\n",
    "| Experiment | Description |\n",
    "|------------|-------------|\n",
    "| **A. Standard Split** | Random 90/10 train/test split with 5-fold CV on training set |\n",
    "| **A2. Proximity-Controlled** | Split that prevents \"near\" leakage between train/test |\n",
    "| **B. Cold-Start Households** | 20% of households held out entirely (inductive evaluation) |\n",
    "| **C. Cold-Start Schools** | 20% of schools held out entirely (inductive evaluation) |\n",
    "\n",
    "Candidate pools are made explicit: **ALL-candidate** vs. **UNSEEN-only** for fair comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## Output Structure\n",
    "\n",
    "```\n",
    "outputs/\n",
    "└── ROBUST_YYYYMMDD_HHMMSS/\n",
    "    ├── A_standard/\n",
    "    │   └── robustness_summary.json\n",
    "    ├── B_coldstart_households/\n",
    "    │   └── robustness_summary.json\n",
    "    ├── C_coldstart_schools/\n",
    "    │   └── robustness_summary.json\n",
    "    ├── robustness_table_min.json\n",
    "    └── robustness_table_min.tex\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## License\n",
    "\n",
    "This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n",
    "\n",
    "---\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "This research was partially supported by the **National Institute of Standards and Technology (NIST) Center of Excellence for Risk-Based Community Resilience Planning** through a cooperative agreement with Colorado State University (Grant Numbers: 70NANB20H008 and 70NANB15H044), the **CSU Pueblo Foundation**, and the **School of Engineering at Colorado State University Pueblo**.\n",
    "\n",
    "We thank **Dr. Nathanael Rosenheim** for curating and sharing the Housing Unit Inventory (HUI) dataset and replication code on DesignSafe-CI (Project PRJ-2961), and the DesignSafe-CI and IN-CORE teams for data hosting, curation, and research infrastructure support. We also appreciate the Housing Unit Allocation (HUA) and Person-Record (PREC) workflow maintainers (Dr. N. Rosenheim, M. Safayet, Dr. A. Beck) for open-sourcing their tools.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d620d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:03:37] [setup] Using device: cpu | RAM=2.69 GB\n",
      "[22:03:37] loading CSVs | RAM=2.69 GB\n",
      "[22:03:37] data loaded and deduplicated | RAM=2.71 GB\n",
      "[22:03:37] graph built | RAM=2.71 GB\n",
      "[22:03:37] [Robust] Data loaded | RAM=2.70 GB\n",
      "[22:03:37] [Robust] Stage 1 pretraining on dev_graph | RAM=2.70 GB\n",
      "  [Pretrain 005] recon_att:0.6894 recon_hh:0.6613 recon_ss:0.0805 info_nce:4.0929\n",
      "  [Pretrain 010] recon_att:0.6771 recon_hh:0.6606 recon_ss:0.0519 info_nce:3.8217\n",
      "  [Pretrain 015] recon_att:0.6396 recon_hh:0.4773 recon_ss:0.0003 info_nce:4.1793\n",
      "  [Pretrain 020] recon_att:0.5954 recon_hh:0.5161 recon_ss:0.0004 info_nce:3.7503\n",
      "  [Pretrain 025] recon_att:0.5787 recon_hh:0.2786 recon_ss:0.0013 info_nce:3.8262\n",
      "  [Pretrain 030] recon_att:0.5598 recon_hh:0.0752 recon_ss:0.0004 info_nce:3.8395\n",
      "  [Pretrain 035] recon_att:0.5905 recon_hh:0.3401 recon_ss:0.0000 info_nce:3.7592\n",
      "  [Pretrain 040] recon_att:0.5881 recon_hh:0.5031 recon_ss:0.0001 info_nce:3.7440\n",
      "  [Pretrain 045] recon_att:0.5627 recon_hh:0.4496 recon_ss:0.0018 info_nce:3.8016\n",
      "  [Pretrain 050] recon_att:0.5647 recon_hh:0.3339 recon_ss:0.0048 info_nce:3.8758\n",
      "  [Pretrain 055] recon_att:0.5588 recon_hh:0.1995 recon_ss:0.0029 info_nce:3.7535\n",
      "  [Pretrain 060] recon_att:0.5472 recon_hh:0.0777 recon_ss:0.0022 info_nce:3.7901\n",
      "  [Pretrain 065] recon_att:0.5577 recon_hh:0.0384 recon_ss:0.0051 info_nce:3.7131\n",
      "  [Pretrain 070] recon_att:0.5312 recon_hh:0.0380 recon_ss:0.0039 info_nce:3.6617\n",
      "  [Pretrain 075] recon_att:0.5263 recon_hh:0.0235 recon_ss:0.0020 info_nce:3.6236\n",
      "  [Pretrain 080] recon_att:0.4817 recon_hh:0.0214 recon_ss:0.0005 info_nce:3.5998\n",
      "[22:05:16] [Robust] Computing pools on dev_graph | RAM=3.04 GB\n",
      "[22:08:20] [Robust] Stage 1 pretraining on dev_graph | RAM=3.05 GB\n",
      "  [Pretrain 005] recon_att:0.6918 recon_hh:0.6594 recon_ss:0.0767 info_nce:4.0918\n",
      "  [Pretrain 010] recon_att:0.6758 recon_hh:0.6558 recon_ss:0.0560 info_nce:3.8029\n",
      "  [Pretrain 015] recon_att:0.6721 recon_hh:0.4094 recon_ss:0.1958 info_nce:4.4926\n",
      "  [Pretrain 020] recon_att:0.6242 recon_hh:0.5219 recon_ss:0.0005 info_nce:3.7443\n",
      "  [Pretrain 025] recon_att:0.6195 recon_hh:0.2498 recon_ss:0.0020 info_nce:3.9686\n",
      "  [Pretrain 030] recon_att:0.7457 recon_hh:0.0789 recon_ss:0.0037 info_nce:4.0074\n",
      "  [Pretrain 035] recon_att:0.5888 recon_hh:0.2715 recon_ss:0.0001 info_nce:3.7824\n",
      "  [Pretrain 040] recon_att:0.5977 recon_hh:0.1695 recon_ss:0.0007 info_nce:3.8507\n",
      "  [Pretrain 045] recon_att:0.5612 recon_hh:0.0724 recon_ss:0.0003 info_nce:3.7951\n",
      "  [Pretrain 050] recon_att:0.5637 recon_hh:0.0246 recon_ss:0.0002 info_nce:3.8257\n",
      "  [Pretrain 055] recon_att:0.5311 recon_hh:0.0558 recon_ss:0.0002 info_nce:3.7941\n",
      "  [Pretrain 060] recon_att:0.5382 recon_hh:0.0987 recon_ss:0.0001 info_nce:3.7928\n",
      "  [Pretrain 065] recon_att:0.5418 recon_hh:0.0526 recon_ss:0.0000 info_nce:3.8254\n",
      "  [Pretrain 070] recon_att:0.5257 recon_hh:0.0152 recon_ss:0.0000 info_nce:3.7922\n",
      "  [Pretrain 075] recon_att:0.5138 recon_hh:0.0059 recon_ss:0.0000 info_nce:3.8265\n",
      "  [Pretrain 080] recon_att:0.5136 recon_hh:0.0043 recon_ss:0.0000 info_nce:3.8175\n",
      "[22:12:22] [Robust] Computing pools on dev_graph | RAM=3.23 GB\n",
      "[22:13:18] [Robust] Stage 1 pretraining on dev_graph | RAM=2.64 GB\n",
      "  [Pretrain 005] recon_att:0.6781 recon_hh:0.6616 recon_ss:0.1514 info_nce:3.9573\n",
      "  [Pretrain 010] recon_att:0.6342 recon_hh:0.6446 recon_ss:0.0418 info_nce:3.5826\n",
      "  [Pretrain 015] recon_att:0.8579 recon_hh:0.6168 recon_ss:0.0000 info_nce:3.8907\n",
      "  [Pretrain 020] recon_att:0.6579 recon_hh:0.6244 recon_ss:0.0007 info_nce:3.6713\n",
      "  [Pretrain 025] recon_att:0.6369 recon_hh:0.5536 recon_ss:0.0268 info_nce:3.6247\n",
      "  [Pretrain 030] recon_att:0.5634 recon_hh:0.2269 recon_ss:0.0097 info_nce:3.6909\n",
      "  [Pretrain 035] recon_att:0.5703 recon_hh:0.2671 recon_ss:0.0533 info_nce:3.9369\n",
      "  [Pretrain 040] recon_att:0.5889 recon_hh:0.4720 recon_ss:0.0001 info_nce:3.6875\n",
      "  [Pretrain 045] recon_att:0.5753 recon_hh:0.4277 recon_ss:0.0000 info_nce:3.6796\n",
      "  [Pretrain 050] recon_att:0.5315 recon_hh:0.1966 recon_ss:0.0000 info_nce:3.6833\n",
      "  [Pretrain 055] recon_att:0.5724 recon_hh:0.0126 recon_ss:0.0000 info_nce:3.7016\n",
      "  [Pretrain 060] recon_att:0.5101 recon_hh:0.0471 recon_ss:0.0000 info_nce:3.6517\n",
      "  [Pretrain 065] recon_att:0.5144 recon_hh:0.1034 recon_ss:0.0000 info_nce:3.6559\n",
      "  [Pretrain 070] recon_att:0.5134 recon_hh:0.0631 recon_ss:0.0000 info_nce:3.6567\n",
      "  [Pretrain 075] recon_att:0.4915 recon_hh:0.0152 recon_ss:0.0000 info_nce:3.7211\n",
      "  [Pretrain 080] recon_att:0.4761 recon_hh:0.0040 recon_ss:0.0000 info_nce:3.6590\n",
      "[22:17:31] [Robust] Computing pools on dev_graph | RAM=3.37 GB\n",
      "\n",
      "=== Robustness Summary ===\n",
      "Standard test: AUC=0.9578, AP=0.9416\n",
      "  Leakage ablation: n/a (no works_at edges present)\n",
      "Cold-start households: AUC=0.8641, AP=0.7515\n",
      "Cold-start schools:    AUC=0.9974, AP=0.9968\n",
      "\n",
      "[Robustness artifacts saved under] ./outputs\\ROBUST_20260201_220337\n",
      "LaTeX table: ./outputs\\ROBUST_20260201_220337\\robustness_table_min.tex\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "GEO-SOCIAL GRAPH NEURAL NETWORK FOR SCHOOL ATTENDANCE LINK PREDICTION\n",
    "================================================================================\n",
    "\n",
    "PAPER REFERENCE\n",
    "---------------\n",
    "This is the official implementation accompanying the paper:\n",
    "\n",
    "    \"Calibrated geo-social link prediction for household–school connectivity\n",
    "     in community resilience\"\n",
    "    Gupta, H.S., Biswas, S., & Nicholson, C.D.\n",
    "    International Journal of Disaster Risk Reduction, Volume 131 (2025)\n",
    "    DOI: https://doi.org/10.1016/j.ijdrr.2025.105872\n",
    "\n",
    "ABSTRACT\n",
    "--------\n",
    "This module implements a complete machine learning pipeline for predicting \n",
    "household-school attendance relationships using heterogeneous graph neural \n",
    "networks. The framework is designed to support disaster impact assessment\n",
    "and emergency planning by accurately reconstructing school enrollment patterns\n",
    "from synthetic population data.\n",
    "\n",
    "MODEL ARCHITECTURE\n",
    "------------------\n",
    "The model combines four key components:\n",
    "\n",
    "    1. FEATURE ENCODER\n",
    "       - MLP-based encoder with Fourier positional encoding for geographic\n",
    "         coordinates (captures high-frequency spatial patterns)\n",
    "       - Categorical embeddings for demographic attributes (ownership, race,\n",
    "         Hispanic origin)\n",
    "       \n",
    "    2. HETEROGENEOUS GRAPH TRANSFORMER (HGT)\n",
    "       - Multi-head attention across heterogeneous node and edge types\n",
    "       - Learns semantic representations from node features and graph structure\n",
    "       - Reference: Hu et al. (2020) WWW\n",
    "       \n",
    "    3. LIGHTGCN BRANCH\n",
    "       - Simplified graph convolution for collaborative filtering\n",
    "       - Operates on bipartite household-school graph\n",
    "       - Reference: He et al. (2020) SIGIR\n",
    "       \n",
    "    4. FUSION GATE\n",
    "       - Learned gating mechanism to adaptively combine HGT and LightGCN\n",
    "       - Balances content-based and collaborative filtering signals\n",
    "\n",
    "TRAINING PIPELINE\n",
    "-----------------\n",
    "    Stage 1: Self-supervised pre-training with denoising reconstruction + InfoNCE\n",
    "    Stage 2: Frozen backbone fine-tuning with hard negative sampling\n",
    "    Stage 3: (Optional) Joint fine-tuning with unfrozen backbone\n",
    "    Stage 4: Evaluation on held-out test set\n",
    "\n",
    "ROBUSTNESS EXPERIMENTS\n",
    "----------------------\n",
    "    A. Standard holdout (random 90/10 split) with leakage ablation\n",
    "    B. Cold-start households (20% of households unseen during training)\n",
    "    C. Cold-start schools (20% of schools unseen during training)\n",
    "\n",
    "Authors: Himadri Sen Gupta, Saptadeep Biswas, Charles D. Nicholson\n",
    "Version: 1.0.0\n",
    "License: MIT\n",
    "Python: >= 3.8\n",
    "Dependencies: PyTorch>=2.0, PyTorch Geometric>=2.3, torch-cluster, scikit-learn\n",
    "\n",
    "Usage:\n",
    "    # From command line:\n",
    "    $ python geo_social_school_AI.py --mode robust --seed 42\n",
    "    \n",
    "    # From Jupyter/Python:\n",
    "    >>> paths = {'households': '...', 'students': '...', 'staff': '...'}\n",
    "    >>> run_robustness_suite(paths, device='cuda', seed=42)\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: IMPORTS AND DEPENDENCIES\n",
    "# ==============================================================================\n",
    "\n",
    "# Standard library imports for system operations and utilities\n",
    "import os                    # Operating system interface for file/path operations\n",
    "import time                  # Time-related functions for logging timestamps\n",
    "import json                  # JSON encoding/decoding for saving results\n",
    "import math                  # Mathematical functions (pi, sqrt, etc.)\n",
    "import random                # Random number generation for reproducibility\n",
    "import gc                    # Garbage collector for memory management\n",
    "import argparse              # Command-line argument parsing\n",
    "import sys                   # System-specific parameters and functions\n",
    "from copy import deepcopy    # Deep copy objects (for model state saving)\n",
    "from datetime import datetime # Date/time handling for experiment naming\n",
    "from typing import Dict, Tuple # Type hints for better code documentation\n",
    "\n",
    "# Numerical computing and data manipulation\n",
    "import numpy as np           # Numerical arrays and mathematical operations\n",
    "import pandas as pd          # DataFrames for tabular data processing\n",
    "\n",
    "# Deep learning framework (PyTorch)\n",
    "import torch                 # Core PyTorch tensor library\n",
    "import torch.nn as nn        # Neural network modules and layers\n",
    "import torch.nn.functional as F  # Functional operations (activation, loss, etc.)\n",
    "\n",
    "# Evaluation metrics from scikit-learn\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,           # Area Under ROC Curve (discrimination ability)\n",
    "    average_precision_score, # Average Precision (AP) for imbalanced data\n",
    "    brier_score_loss         # Brier score (calibration + refinement)\n",
    ")\n",
    "\n",
    "# PyTorch Geometric for graph neural networks\n",
    "from torch_geometric.data import HeteroData       # Heterogeneous graph container\n",
    "from torch_geometric.transforms import ToUndirected, RandomLinkSplit  # Graph transforms\n",
    "from torch_geometric.nn import HGTConv            # Heterogeneous Graph Transformer\n",
    "from torch_cluster import knn_graph               # K-nearest neighbors graph construction\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: OPTIONAL DEPENDENCIES\n",
    "# ==============================================================================\n",
    "\n",
    "# psutil for RAM monitoring (optional but recommended for large datasets)\n",
    "try:\n",
    "    import psutil  # Process and system utilities for memory tracking\n",
    "except ImportError:\n",
    "    psutil = None  # Gracefully handle missing dependency\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: LOGGING AND REPRODUCIBILITY UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def tick(msg: str) -> None:\n",
    "    \"\"\"\n",
    "    Print a timestamped log message with optional memory usage.\n",
    "    \n",
    "    This function provides consistent logging throughout the pipeline,\n",
    "    helping track progress and identify memory bottlenecks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    msg : str\n",
    "        The message to display in the log output.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Prints directly to stdout with flush for immediate display.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> tick(\"Loading dataset\")\n",
    "    [14:32:15] Loading dataset | RAM=2.34 GB\n",
    "    \"\"\"\n",
    "    # Check if psutil is available for memory monitoring\n",
    "    if psutil is not None:\n",
    "        # Get current process memory usage in gigabytes\n",
    "        rss_gb = psutil.Process(os.getpid()).memory_info().rss / (1024**3)\n",
    "        # Print with timestamp, message, and memory usage\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] {msg} | RAM={rss_gb:.2f} GB\", flush=True)\n",
    "    else:\n",
    "        # Print without memory info if psutil unavailable\n",
    "        print(f\"[{time.strftime('%H:%M:%S')}] {msg}\", flush=True)\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Set random seeds for reproducibility across all libraries.\n",
    "    \n",
    "    Ensures deterministic behavior by setting seeds for:\n",
    "    - Python's random module\n",
    "    - NumPy's random number generator\n",
    "    - PyTorch's CPU random number generator\n",
    "    - PyTorch's CUDA random number generators (if available)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The seed value for random number generators.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Seeds are set globally for all random operations.\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    For full reproducibility with CUDA, you may also need:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> set_seed(42)  # Set all seeds to 42\n",
    "    \"\"\"\n",
    "    # Set Python's built-in random module seed\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Set NumPy's random number generator seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Set PyTorch's CPU random number generator seed\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Set CUDA seeds if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)  # Set for all GPUs\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: DATA PREPROCESSING UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def zscore(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute z-score normalization (standardization) for numerical features.\n",
    "    \n",
    "    Z-score normalization transforms data to have zero mean and unit variance,\n",
    "    which helps neural networks converge faster and more reliably.\n",
    "    \n",
    "    Formula: z = (x - μ) / σ\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Input array of numerical values to normalize.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Z-score normalized array with same shape as input.\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    Handles edge cases:\n",
    "    - NaN values are preserved (not modified)\n",
    "    - Zero or NaN standard deviation defaults to 1.0 to avoid division errors\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> arr = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    >>> zscore(arr)\n",
    "    array([-1.41421356, -0.70710678, 0., 0.70710678, 1.41421356])\n",
    "    \"\"\"\n",
    "    # Convert to float64 for numerical precision\n",
    "    x = x.astype('float64')\n",
    "    \n",
    "    # Compute mean ignoring NaN values\n",
    "    m = np.nanmean(x)\n",
    "    \n",
    "    # Compute standard deviation ignoring NaN values\n",
    "    s = np.nanstd(x)\n",
    "    \n",
    "    # Handle edge case: zero or invalid standard deviation\n",
    "    # Set to 1.0 to avoid division by zero (effectively just centers the data)\n",
    "    s = 1.0 if (not np.isfinite(s) or s == 0) else s\n",
    "    \n",
    "    # Return standardized values\n",
    "    return (x - m) / s\n",
    "\n",
    "\n",
    "def factorize_col(series: pd.Series) -> Tuple[np.ndarray, list]:\n",
    "    \"\"\"\n",
    "    Convert categorical column to integer indices with category mapping.\n",
    "    \n",
    "    This function creates a mapping from categorical values to consecutive\n",
    "    integer indices, which is required for embedding layers in neural networks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Pandas Series containing categorical values (any dtype).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, list]\n",
    "        - inv : Integer indices (int64) corresponding to each row\n",
    "        - cats : List of unique category labels in sorted order\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> s = pd.Series(['cat', 'dog', 'cat', 'bird'])\n",
    "    >>> indices, categories = factorize_col(s)\n",
    "    >>> indices\n",
    "    array([1, 2, 1, 0], dtype=int64)\n",
    "    >>> categories\n",
    "    ['bird', 'cat', 'dog']\n",
    "    \"\"\"\n",
    "    # Convert all values to strings for consistent handling\n",
    "    s = series.astype(str).values\n",
    "    \n",
    "    # Get unique categories and inverse indices\n",
    "    # np.unique returns sorted unique values and indices to reconstruct original\n",
    "    cats, inv = np.unique(s, return_inverse=True)\n",
    "    \n",
    "    # Return integer indices and list of category labels\n",
    "    return inv.astype('int64'), cats.tolist()\n",
    "\n",
    "\n",
    "def fourier_features(xy: torch.Tensor, n_freq: int = 4) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate Fourier positional encodings for geographic coordinates.\n",
    "    \n",
    "    Fourier features allow neural networks to learn high-frequency patterns\n",
    "    in spatial data. This is based on the principle that standard MLPs have\n",
    "    difficulty learning high-frequency functions (spectral bias).\n",
    "    \n",
    "    The encoding uses multiple frequency bands:\n",
    "        freq_k = 2^k * π, for k = 0, 1, ..., n_freq-1\n",
    "    \n",
    "    For each coordinate and frequency, we compute:\n",
    "        sin(coord * freq_k) and cos(coord * freq_k)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    xy : torch.Tensor\n",
    "        Input tensor of shape [N, 2] containing (latitude, longitude) pairs.\n",
    "        Coordinates should be z-score normalized for best results.\n",
    "        \n",
    "    n_freq : int, optional (default=4)\n",
    "        Number of frequency bands to use. Total output dimension = 4 * n_freq.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Fourier features of shape [N, 4*n_freq].\n",
    "        Features are concatenated as: [lat_sin, lat_cos, lon_sin, lon_cos]\n",
    "        \n",
    "    Reference\n",
    "    ---------\n",
    "    Tancik et al. (2020) \"Fourier Features Let Networks Learn High Frequency\n",
    "    Functions in Low Dimensional Domains\" (NeurIPS)\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> coords = torch.tensor([[0.5, -0.3], [1.2, 0.8]])\n",
    "    >>> feats = fourier_features(coords, n_freq=4)\n",
    "    >>> feats.shape\n",
    "    torch.Size([2, 16])\n",
    "    \"\"\"\n",
    "    # Extract latitude and longitude columns\n",
    "    lat = xy[:, 0:1]  # Shape: [N, 1]\n",
    "    lon = xy[:, 1:2]  # Shape: [N, 1]\n",
    "    \n",
    "    # Generate frequency bands: [π, 2π, 4π, 8π, ...] for n_freq bands\n",
    "    # Shape: [1, n_freq] for broadcasting\n",
    "    freqs = (\n",
    "        torch.pow(\n",
    "            torch.tensor(2.0, device=xy.device),  # Base 2\n",
    "            torch.arange(n_freq, device=xy.device)  # Exponents [0, 1, ..., n_freq-1]\n",
    "        ).view(1, n_freq) * math.pi  # Multiply by π\n",
    "    )\n",
    "    \n",
    "    # Compute sinusoidal features for latitude\n",
    "    lat_sin = torch.sin(lat * freqs)  # Shape: [N, n_freq]\n",
    "    lat_cos = torch.cos(lat * freqs)  # Shape: [N, n_freq]\n",
    "    \n",
    "    # Compute sinusoidal features for longitude\n",
    "    lon_sin = torch.sin(lon * freqs)  # Shape: [N, n_freq]\n",
    "    lon_cos = torch.cos(lon * freqs)  # Shape: [N, n_freq]\n",
    "    \n",
    "    # Concatenate all features along the last dimension\n",
    "    # Final shape: [N, 4*n_freq]\n",
    "    return torch.cat([lat_sin, lat_cos, lon_sin, lon_cos], dim=1)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 5: DATA LOADING AND GRAPH CONSTRUCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def load_and_build(paths: Dict[str, str]) -> Tuple[HeteroData, Dict, Dict, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load CSV data and construct a heterogeneous graph for link prediction.\n",
    "    \n",
    "    This function performs the complete ETL (Extract, Transform, Load) pipeline:\n",
    "    1. Load household, student, and staff CSV files\n",
    "    2. Clean and preprocess data (handle missing values, merge coordinates)\n",
    "    3. Create node features for households and schools\n",
    "    4. Build edge indices for multiple relationship types\n",
    "    5. Construct spatial proximity edges using KNN\n",
    "    6. Return a PyTorch Geometric HeteroData object\n",
    "    \n",
    "    Graph Structure\n",
    "    ---------------\n",
    "    Node Types:\n",
    "        - 'household': Residential units with demographic features\n",
    "        - 'school': Educational institutions with location features\n",
    "        \n",
    "    Edge Types:\n",
    "        - ('household', 'attends', 'school'): Student enrollment\n",
    "        - ('household', 'works_at', 'school'): Staff employment (optional)\n",
    "        - ('household', 'spatially_near', 'household'): Geographic proximity (KNN)\n",
    "        - ('school', 'near', 'school'): School proximity (KNN)\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    paths : Dict[str, str]\n",
    "        Dictionary containing file paths:\n",
    "        - 'households': Path to household CSV file\n",
    "        - 'students': Path to student CSV file\n",
    "        - 'staff': Path to staff CSV file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple containing:\n",
    "        - data : HeteroData\n",
    "            PyTorch Geometric heterogeneous graph object\n",
    "        - sizes : Dict\n",
    "            Vocabulary sizes for categorical embeddings\n",
    "            {'n_own': int, 'n_race': int, 'n_his': int}\n",
    "        - maps : Dict\n",
    "            ID to index mappings for households and schools\n",
    "        - df_house : pd.DataFrame\n",
    "            Cleaned household DataFrame\n",
    "        - df_schools : pd.DataFrame\n",
    "            Cleaned schools DataFrame\n",
    "            \n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If any required CSV file is not found.\n",
    "    KeyError\n",
    "        If required columns are missing from CSV files.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> paths = {\n",
    "    ...     'households': 'hui_data.csv',\n",
    "    ...     'students': 'students_data.csv',\n",
    "    ...     'staff': 'staff_data.csv'\n",
    "    ... }\n",
    "    >>> data, sizes, maps, df_h, df_s = load_and_build(paths)\n",
    "    >>> data.node_types\n",
    "    ['household', 'school']\n",
    "    \"\"\"\n",
    "    # Log the start of data loading\n",
    "    tick(\"loading CSVs\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Load raw CSV files\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_hui = pd.read_csv(paths['households'])      # Household unit information\n",
    "    df_students = pd.read_csv(paths['students'])   # Student enrollment records\n",
    "    df_staff = pd.read_csv(paths['staff'])         # Staff employment records\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: Clean household data (remove rows with missing required columns)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Required columns for household features\n",
    "    need_cols = ['ownershp', 'race', 'hispan', 'randincome']\n",
    "    \n",
    "    # Drop rows with any missing values in required columns\n",
    "    df_clean = df_hui.dropna(subset=need_cols).copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: Merge geographic coordinates from students/staff to households\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract unique household coordinates from student records\n",
    "    student_coords = df_students[['huid', 'hcb_lat', 'hcb_lon']].drop_duplicates('huid')\n",
    "    \n",
    "    # Extract unique household coordinates from staff records\n",
    "    staff_coords = df_staff[['huid', 'hcb_lat', 'hcb_lon']].drop_duplicates('huid')\n",
    "    \n",
    "    # Combine coordinate sources, prioritizing first occurrence\n",
    "    all_coords = (\n",
    "        pd.concat([student_coords, staff_coords], ignore_index=True)\n",
    "        .drop_duplicates('huid')\n",
    "        .set_index('huid')\n",
    "    )\n",
    "    \n",
    "    # Join coordinates to household data\n",
    "    df_clean = df_clean.join(all_coords, on='huid')\n",
    "    \n",
    "    # Remove households without valid coordinates\n",
    "    df_clean.dropna(subset=['hcb_lat', 'hcb_lon'], inplace=True)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 4: Build school reference table\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Extract unique schools from student data\n",
    "    df_schools = df_students[['NCESSCH', 'SCHNAM09', 'ncs_lat', 'ncs_lon']].drop_duplicates('NCESSCH')\n",
    "    \n",
    "    # Create school name to ID mapping for staff records\n",
    "    school_name_to_id = df_schools.set_index('SCHNAM09')['NCESSCH']\n",
    "    \n",
    "    # Map school names to IDs in staff data (if applicable)\n",
    "    if 'SIName' in df_staff.columns:\n",
    "        df_staff['NCESSCH'] = df_staff['SIName'].map(school_name_to_id)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 5: Create index mappings for graph construction\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Get sorted lists of unique IDs\n",
    "    household_ids = sorted(df_clean['huid'].unique().tolist())\n",
    "    school_ids = sorted(df_schools['NCESSCH'].unique().tolist())\n",
    "    \n",
    "    # Create dictionaries mapping IDs to consecutive integer indices\n",
    "    household_map = {hid: i for i, hid in enumerate(household_ids)}\n",
    "    school_map = {sid: i for i, sid in enumerate(school_ids)}\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 6: Build 'attends' edges (student enrollment)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Filter students to only include valid households and schools\n",
    "    df_students_f = df_students[\n",
    "        df_students['huid'].isin(household_map) & \n",
    "        df_students['NCESSCH'].isin(school_map)\n",
    "    ]\n",
    "    \n",
    "    # Get unique (household, school) pairs\n",
    "    pairs_students = df_students_f[['huid', 'NCESSCH']].dropna().drop_duplicates().values\n",
    "    \n",
    "    # Convert to graph indices\n",
    "    att_src = [household_map[h] for h, s in pairs_students]  # Source: household indices\n",
    "    att_dst = [school_map[s] for h, s in pairs_students]     # Target: school indices\n",
    "    \n",
    "    # Create edge index tensor [2, num_edges]\n",
    "    att_ei = torch.tensor([att_src, att_dst], dtype=torch.long)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 7: Build 'works_at' edges (staff employment) - optional\n",
    "    # -------------------------------------------------------------------------\n",
    "    wrk_ei = None  # Initialize as None\n",
    "    \n",
    "    if 'NCESSCH' in df_staff.columns:\n",
    "        # Filter staff to valid schools\n",
    "        df_staff_f = df_staff.dropna(subset=['NCESSCH'])\n",
    "        df_staff_f = df_staff_f[\n",
    "            df_staff_f['huid'].isin(household_map) & \n",
    "            df_staff_f['NCESSCH'].isin(school_map)\n",
    "        ]\n",
    "        \n",
    "        # Get unique (household, school) employment pairs\n",
    "        pairs_staff = df_staff_f[['huid', 'NCESSCH']].drop_duplicates().values\n",
    "        \n",
    "        if len(pairs_staff) > 0:\n",
    "            wrk_src = [household_map[h] for h, s in pairs_staff]\n",
    "            wrk_dst = [school_map[s] for h, s in pairs_staff]\n",
    "            wrk_ei = torch.tensor([wrk_src, wrk_dst], dtype=torch.long)\n",
    "\n",
    "    # Log completion of data loading\n",
    "    tick(\"data loaded and deduplicated\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 8: Create household node features\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Reindex household DataFrame to match graph ordering\n",
    "    df_house = df_clean.set_index('huid').loc[household_ids].reset_index()\n",
    "    \n",
    "    # Factorize categorical variables for embedding layers\n",
    "    own_ids, own_cats = factorize_col(df_house['ownershp'])  # Ownership status\n",
    "    rac_ids, rac_cats = factorize_col(df_house['race'])       # Race/ethnicity\n",
    "    his_ids, his_cats = factorize_col(df_house['hispan'])     # Hispanic origin\n",
    "\n",
    "    # Create numerical feature tensor (z-score normalized)\n",
    "    # Features: [income, household_size, latitude, longitude]\n",
    "    num_feats_house = torch.tensor(\n",
    "        np.vstack([\n",
    "            zscore(df_house['randincome'].values),  # Household income\n",
    "            zscore(df_house['numprec'].values),     # Number of persons in household\n",
    "            zscore(df_house['hcb_lat'].values),     # Latitude (normalized)\n",
    "            zscore(df_house['hcb_lon'].values)      # Longitude (normalized)\n",
    "        ]).T,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 9: Create school node features\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Reindex schools DataFrame to match graph ordering\n",
    "    df_schools_sorted = df_schools.set_index('NCESSCH').loc[school_ids].reset_index()\n",
    "    \n",
    "    # Create numerical feature tensor (z-score normalized coordinates)\n",
    "    num_feats_school = torch.tensor(\n",
    "        np.vstack([\n",
    "            zscore(df_schools_sorted['ncs_lat'].values),  # School latitude\n",
    "            zscore(df_schools_sorted['ncs_lon'].values)   # School longitude\n",
    "        ]).T,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 10: Construct HeteroData graph object\n",
    "    # -------------------------------------------------------------------------\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # Add household node features\n",
    "    data['household'].x = num_feats_house           # Numerical features [N_h, 4]\n",
    "    data['household'].own_idx = torch.tensor(own_ids, dtype=torch.long)  # Ownership indices\n",
    "    data['household'].rac_idx = torch.tensor(rac_ids, dtype=torch.long)  # Race indices\n",
    "    data['household'].his_idx = torch.tensor(his_ids, dtype=torch.long)  # Hispanic indices\n",
    "    \n",
    "    # Add school node features\n",
    "    data['school'].x = num_feats_school             # Numerical features [N_s, 2]\n",
    "\n",
    "    # Add edge types\n",
    "    data[('household', 'attends', 'school')].edge_index = att_ei\n",
    "    \n",
    "    if wrk_ei is not None and wrk_ei.numel() > 0:\n",
    "        data[('household', 'works_at', 'school')].edge_index = wrk_ei\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 11: Build spatial proximity edges using K-Nearest Neighbors\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Household-to-household spatial edges (k=8 neighbors)\n",
    "    hh_coords = data['household'].x[:, -2:]  # Extract z-scored lat/lon\n",
    "    data[('household', 'spatially_near', 'household')].edge_index = knn_graph(\n",
    "        hh_coords, k=8, loop=False\n",
    "    )\n",
    "\n",
    "    # School-to-school spatial edges (k=4 neighbors)\n",
    "    sc_coords = data['school'].x\n",
    "    data[('school', 'near', 'school')].edge_index = knn_graph(\n",
    "        sc_coords, k=4, loop=False\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 12: Convert directed edges to undirected (add reverse edges)\n",
    "    # -------------------------------------------------------------------------\n",
    "    data = ToUndirected()(data)\n",
    "    \n",
    "    # Log graph construction completion\n",
    "    tick(\"graph built\")\n",
    "    \n",
    "    # Prepare vocabulary sizes for embedding layers\n",
    "    sizes = dict(\n",
    "        n_own=len(own_cats),   # Number of ownership categories\n",
    "        n_race=len(rac_cats),  # Number of race categories\n",
    "        n_his=len(his_cats)    # Number of Hispanic origin categories\n",
    "    )\n",
    "    \n",
    "    return data, sizes, dict(household=household_map, school=school_map), df_house, df_schools_sorted\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 6: GRAPH SPLITTING AND SANITIZATION UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def sanitize_for_resplit(g: HeteroData) -> HeteroData:\n",
    "    \"\"\"\n",
    "    Create a clean copy of the graph with only node features and edge indices.\n",
    "    \n",
    "    This function removes all edge labels, masks, and split-related attributes\n",
    "    that may have been added by previous RandomLinkSplit calls. This is necessary\n",
    "    before applying a new split to avoid contamination.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g : HeteroData\n",
    "        Input heterogeneous graph, possibly with edge labels/masks.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    HeteroData\n",
    "        Clean graph containing only:\n",
    "        - Node attributes (features, indices, etc.)\n",
    "        - Edge indices (no edge labels or masks)\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    This prevents data leakage when creating nested train/val splits\n",
    "    from an already-split dataset.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> # After a split, graph has edge_label attributes\n",
    "    >>> clean_graph = sanitize_for_resplit(split_graph)\n",
    "    >>> hasattr(clean_graph[('household','attends','school')], 'edge_label')\n",
    "    False\n",
    "    \"\"\"\n",
    "    # Create new empty HeteroData object\n",
    "    ng = HeteroData()\n",
    "    \n",
    "    # Copy all node-level attributes (features, indices, etc.)\n",
    "    for nt in g.node_types:\n",
    "        for k, v in g[nt].items():\n",
    "            ng[nt][k] = v\n",
    "            \n",
    "    # Copy only edge_index (exclude edge_label, edge_label_index, masks)\n",
    "    for et in g.edge_types:\n",
    "        if 'edge_index' in g[et]:\n",
    "            ng[et].edge_index = g[et].edge_index\n",
    "            \n",
    "    return ng\n",
    "\n",
    "\n",
    "def make_fixed_test_holdout(\n",
    "    data: HeteroData, \n",
    "    test_ratio: float = 0.10, \n",
    "    seed: int = 42\n",
    ") -> Tuple[HeteroData, HeteroData]:\n",
    "    \"\"\"\n",
    "    Create a fixed test set holdout from the full graph.\n",
    "    \n",
    "    This function performs a single train/test split on the 'attends' edges,\n",
    "    holding out a fraction for final evaluation. The test set remains fixed\n",
    "    across all experiments to ensure comparable results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : HeteroData\n",
    "        Full heterogeneous graph with all edges.\n",
    "        \n",
    "    test_ratio : float, optional (default=0.10)\n",
    "        Fraction of edges to hold out for testing (0.0 to 1.0).\n",
    "        \n",
    "    seed : int, optional (default=42)\n",
    "        Random seed for reproducible splitting.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[HeteroData, HeteroData]\n",
    "        - dev_graph: Training/validation graph (90% of edges by default)\n",
    "        - test_holdout: Test split with edge labels\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    The dev_graph is sanitized (edge labels removed) so it can be\n",
    "    re-split for cross-validation without contamination.\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> dev_graph, test_split = make_fixed_test_holdout(data, test_ratio=0.15)\n",
    "    >>> # dev_graph has ~85% of edges, test_split has ~15%\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducible split\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Configure RandomLinkSplit transform\n",
    "    t = RandomLinkSplit(\n",
    "        num_val=0.0,              # No validation in this split\n",
    "        num_test=test_ratio,      # Specified test ratio\n",
    "        is_undirected=True,       # Graph is undirected\n",
    "        add_negative_train_samples=False,  # Don't add negatives yet\n",
    "        edge_types=[('household', 'attends', 'school')],  # Target edge type\n",
    "        rev_edge_types=[('school', 'rev_attends', 'household')],  # Reverse edges\n",
    "        split_labels=True         # Create edge_label and edge_label_index\n",
    "    )\n",
    "    \n",
    "    # Apply split (returns train, val, test - we ignore val since num_val=0)\n",
    "    dev_graph, _, test_holdout = t(data)\n",
    "    \n",
    "    # Clean dev_graph for subsequent nested splits\n",
    "    dev_graph = sanitize_for_resplit(dev_graph)\n",
    "    \n",
    "    return dev_graph, test_holdout\n",
    "\n",
    "\n",
    "def make_k_dev_folds(\n",
    "    dev_graph: HeteroData, \n",
    "    k: int = 5, \n",
    "    base_seed: int = 1000, \n",
    "    val_ratio: float = 0.10\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Create K train/validation folds for cross-validation.\n",
    "    \n",
    "    This function generates K different random splits of the development graph,\n",
    "    each with a different validation set. This enables robust model selection\n",
    "    and hyperparameter tuning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dev_graph : HeteroData\n",
    "        Development graph (after test holdout).\n",
    "        \n",
    "    k : int, optional (default=5)\n",
    "        Number of cross-validation folds.\n",
    "        \n",
    "    base_seed : int, optional (default=1000)\n",
    "        Base random seed. Fold i uses seed = base_seed + i.\n",
    "        \n",
    "    val_ratio : float, optional (default=0.10)\n",
    "        Fraction of dev edges for validation in each fold.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of Tuple[HeteroData, HeteroData]\n",
    "        List of (train_split, val_split) tuples for each fold.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> folds = make_k_dev_folds(dev_graph, k=5, val_ratio=0.15)\n",
    "    >>> len(folds)\n",
    "    5\n",
    "    >>> train_0, val_0 = folds[0]\n",
    "    \"\"\"\n",
    "    # Ensure clean starting point\n",
    "    dev_graph = sanitize_for_resplit(dev_graph)\n",
    "    \n",
    "    folds = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Use unique seed for each fold\n",
    "        set_seed(base_seed + i)\n",
    "        \n",
    "        # Configure split for this fold\n",
    "        t = RandomLinkSplit(\n",
    "            num_val=val_ratio,\n",
    "            num_test=0.0,         # No test split (already held out)\n",
    "            is_undirected=True,\n",
    "            add_negative_train_samples=False,\n",
    "            edge_types=[('household', 'attends', 'school')],\n",
    "            rev_edge_types=[('school', 'rev_attends', 'household')],\n",
    "            split_labels=True\n",
    "        )\n",
    "        \n",
    "        # Apply split\n",
    "        train_i, val_i, _ = t(dev_graph)\n",
    "        \n",
    "        folds.append((train_i, val_i))\n",
    "        \n",
    "    return folds\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 7: NEURAL NETWORK MODEL COMPONENTS\n",
    "# ==============================================================================\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode raw node features into dense hidden representations.\n",
    "    \n",
    "    This module handles the initial feature transformation for both\n",
    "    household and school nodes:\n",
    "    \n",
    "    Household Features:\n",
    "        - Numerical: income, household size, lat, lon (4 dims)\n",
    "        - Fourier positional encoding of coordinates (4 * n_freq dims)\n",
    "        - Categorical embeddings: ownership, race, hispanic (40 dims total)\n",
    "        - MLP projection to hidden dimension\n",
    "        \n",
    "    School Features:\n",
    "        - Numerical: lat, lon (2 dims)\n",
    "        - Fourier positional encoding (4 * n_freq dims)\n",
    "        - MLP projection to hidden dimension\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sizes : dict\n",
    "        Vocabulary sizes for categorical embeddings:\n",
    "        {'n_own': int, 'n_race': int, 'n_his': int}\n",
    "        \n",
    "    hidden : int, optional (default=128)\n",
    "        Hidden and output dimension for feature encodings.\n",
    "        \n",
    "    n_freq : int, optional (default=6)\n",
    "        Number of frequency bands for Fourier encoding.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    emb_own : nn.Embedding\n",
    "        Embedding layer for ownership status (16 dims)\n",
    "    emb_rac : nn.Embedding\n",
    "        Embedding layer for race (16 dims)\n",
    "    emb_his : nn.Embedding\n",
    "        Embedding layer for Hispanic origin (8 dims)\n",
    "    house_mlp : nn.Sequential\n",
    "        MLP for household feature projection\n",
    "    school_mlp : nn.Sequential\n",
    "        MLP for school feature projection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sizes: dict, hidden: int = 128, n_freq: int = 6):\n",
    "        \"\"\"Initialize the feature encoder with embedding and MLP layers.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Categorical embedding layers\n",
    "        self.emb_own = nn.Embedding(sizes['n_own'], 16)   # Ownership embedding\n",
    "        self.emb_rac = nn.Embedding(sizes['n_race'], 16)  # Race embedding\n",
    "        self.emb_his = nn.Embedding(sizes['n_his'], 8)    # Hispanic embedding\n",
    "        \n",
    "        # Calculate input dimensions\n",
    "        # Household: 4 numerical + 4*n_freq Fourier + 16+16+8 categorical = 4 + 24 + 40 = 68\n",
    "        h_in = 4 + 4 * n_freq + 16 + 16 + 8\n",
    "        \n",
    "        # School: 2 numerical + 4*n_freq Fourier\n",
    "        s_in = 2 + 4 * n_freq\n",
    "        \n",
    "        # MLP for household features\n",
    "        self.house_mlp = nn.Sequential(\n",
    "            nn.Linear(h_in, hidden * 2),      # First layer expands\n",
    "            nn.LayerNorm(hidden * 2),          # Normalize for stable training\n",
    "            nn.ReLU(inplace=True),             # Non-linearity\n",
    "            nn.Linear(hidden * 2, hidden)      # Project to hidden dim\n",
    "        )\n",
    "        \n",
    "        # MLP for school features\n",
    "        self.school_mlp = nn.Sequential(\n",
    "            nn.Linear(s_in, hidden),\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, hidden)\n",
    "        )\n",
    "        \n",
    "        # Store n_freq for forward pass\n",
    "        self.n_freq = n_freq\n",
    "        \n",
    "    def forward(self, data: HeteroData) -> dict:\n",
    "        \"\"\"\n",
    "        Encode node features to hidden representations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : HeteroData\n",
    "            Input graph with node features.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Dictionary with encoded features:\n",
    "            {'household': Tensor[N_h, hidden], 'school': Tensor[N_s, hidden]}\n",
    "        \"\"\"\n",
    "        # ----- Household Encoding -----\n",
    "        # Generate Fourier positional encoding from coordinates (columns 2:4)\n",
    "        posenc_h = fourier_features(data['household'].x[:, 2:4], n_freq=self.n_freq)\n",
    "        \n",
    "        # Lookup categorical embeddings and concatenate\n",
    "        e_cat = torch.cat([\n",
    "            self.emb_own(data['household'].own_idx),  # [N_h, 16]\n",
    "            self.emb_rac(data['household'].rac_idx),  # [N_h, 16]\n",
    "            self.emb_his(data['household'].his_idx)   # [N_h, 8]\n",
    "        ], dim=-1)  # [N_h, 40]\n",
    "        \n",
    "        # Concatenate all household features\n",
    "        x_h = torch.cat([\n",
    "            data['household'].x,  # Numerical features [N_h, 4]\n",
    "            posenc_h,             # Fourier encoding [N_h, 4*n_freq]\n",
    "            e_cat                 # Categorical embeddings [N_h, 40]\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # ----- School Encoding -----\n",
    "        # Generate Fourier positional encoding\n",
    "        posenc_s = fourier_features(data['school'].x, n_freq=self.n_freq)\n",
    "        \n",
    "        # Concatenate school features\n",
    "        x_s = torch.cat([\n",
    "            data['school'].x,  # Numerical features [N_s, 2]\n",
    "            posenc_s           # Fourier encoding [N_s, 4*n_freq]\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # Apply MLPs and return\n",
    "        return {\n",
    "            'household': self.house_mlp(x_h),\n",
    "            'school': self.school_mlp(x_s)\n",
    "        }\n",
    "\n",
    "\n",
    "class HGTBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Heterogeneous Graph Transformer (HGT) backbone network.\n",
    "    \n",
    "    HGT learns node representations by attending to neighbors of different\n",
    "    types with type-specific attention mechanisms. This allows the model\n",
    "    to capture the semantic differences between edge types (e.g., 'attends'\n",
    "    vs 'works_at' vs 'spatially_near').\n",
    "    \n",
    "    Reference:\n",
    "        Hu et al. (2020) \"Heterogeneous Graph Transformer\" (WWW)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata : tuple\n",
    "        Graph metadata (node_types, edge_types) from HeteroData.\n",
    "        \n",
    "    hidden : int, optional (default=128)\n",
    "        Hidden dimension for graph convolutions.\n",
    "        \n",
    "    out_dim : int, optional (default=128)\n",
    "        Output dimension after final linear layer.\n",
    "        \n",
    "    layers : int, optional (default=3)\n",
    "        Number of HGT convolution layers.\n",
    "        \n",
    "    heads : int, optional (default=4)\n",
    "        Number of attention heads per layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        metadata: tuple, \n",
    "        hidden: int = 128, \n",
    "        out_dim: int = 128, \n",
    "        layers: int = 3, \n",
    "        heads: int = 4\n",
    "    ):\n",
    "        \"\"\"Initialize HGT backbone with specified architecture.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Stack of HGT convolution layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            HGTConv(hidden, hidden, metadata, heads) \n",
    "            for _ in range(layers)\n",
    "        ])\n",
    "        \n",
    "        # Final linear projection to output dimension\n",
    "        self.out = nn.Linear(hidden, out_dim)\n",
    "        \n",
    "    def forward(self, x_dict: dict, edge_index_dict: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Apply HGT convolutions and output projection.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_dict : dict\n",
    "            Node feature dictionary {node_type: features}.\n",
    "            \n",
    "        edge_index_dict : dict\n",
    "            Edge index dictionary {edge_type: edge_index}.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Output embeddings {node_type: embeddings}.\n",
    "        \"\"\"\n",
    "        # Apply each HGT layer sequentially\n",
    "        for conv in self.layers:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            \n",
    "        # Apply output projection to all node types\n",
    "        return {k: self.out(v) for k, v in x_dict.items()}\n",
    "\n",
    "\n",
    "class LightGCNResidual(nn.Module):\n",
    "    \"\"\"\n",
    "    LightGCN-style message passing for bipartite collaborative filtering.\n",
    "    \n",
    "    LightGCN simplifies GCN by removing feature transformation and non-linearity,\n",
    "    keeping only neighborhood aggregation with symmetric normalization.\n",
    "    The final representation averages embeddings across all layers.\n",
    "    \n",
    "    This is particularly effective for recommendation/link prediction tasks\n",
    "    where the graph structure itself carries strong signal.\n",
    "    \n",
    "    Reference:\n",
    "        He et al. (2020) \"LightGCN: Simplifying and Powering Graph Convolution\n",
    "        Network for Recommendation\" (SIGIR)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embedding dimension (must match input dimension).\n",
    "        \n",
    "    layers : int, optional (default=2)\n",
    "        Number of message passing iterations.\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    Unlike standard GCN, LightGCN:\n",
    "    - Has no learnable parameters\n",
    "    - Uses symmetric normalization: 1/sqrt(deg_u * deg_v)\n",
    "    - Averages representations across all layers (including input)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, layers: int = 2):\n",
    "        \"\"\"Initialize LightGCN with specified depth.\"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        H: torch.Tensor, \n",
    "        S: torch.Tensor, \n",
    "        att_edge_index: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Apply LightGCN message passing on bipartite graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        H : torch.Tensor\n",
    "            Household embeddings [N_h, dim].\n",
    "            \n",
    "        S : torch.Tensor\n",
    "            School embeddings [N_s, dim].\n",
    "            \n",
    "        att_edge_index : torch.Tensor\n",
    "            Edge index for 'attends' edges [2, E].\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            Updated (household_emb, school_emb) averaged across layers.\n",
    "        \"\"\"\n",
    "        # Extract source (household) and target (school) indices\n",
    "        row = att_edge_index[0].to(H.device)  # Household indices\n",
    "        col = att_edge_index[1].to(H.device)  # School indices\n",
    "        \n",
    "        # Get number of nodes\n",
    "        h_num, s_num = H.size(0), S.size(0)\n",
    "        \n",
    "        # Compute node degrees for normalization\n",
    "        deg_h = torch.bincount(row, minlength=h_num).float().clamp(min=1.0)\n",
    "        deg_s = torch.bincount(col, minlength=s_num).float().clamp(min=1.0)\n",
    "        \n",
    "        # Symmetric normalization weights: 1 / sqrt(deg_src * deg_dst)\n",
    "        w = 1.0 / torch.sqrt(deg_h[row] * deg_s[col])  # [E]\n",
    "        \n",
    "        # Initialize layer outputs with input embeddings\n",
    "        H_list = [H]\n",
    "        S_list = [S]\n",
    "        \n",
    "        # Current layer representations\n",
    "        h_cur = H\n",
    "        s_cur = S\n",
    "        \n",
    "        # Message passing iterations\n",
    "        for _ in range(self.layers):\n",
    "            # Aggregate messages to households (from schools)\n",
    "            msg_to_h = torch.zeros_like(H)\n",
    "            msg_to_h.index_add_(0, row, S[col] * w.unsqueeze(-1))\n",
    "            \n",
    "            # Aggregate messages to schools (from households)\n",
    "            msg_to_s = torch.zeros_like(S)\n",
    "            msg_to_s.index_add_(0, col, H[row] * w.unsqueeze(-1))\n",
    "            \n",
    "            # Update current representations\n",
    "            h_cur = msg_to_h\n",
    "            s_cur = msg_to_s\n",
    "            \n",
    "            # Store layer outputs\n",
    "            H_list.append(h_cur)\n",
    "            S_list.append(s_cur)\n",
    "        \n",
    "        # Average across all layers (including input)\n",
    "        H_out = torch.stack(H_list, dim=0).mean(dim=0)\n",
    "        S_out = torch.stack(S_list, dim=0).mean(dim=0)\n",
    "        \n",
    "        return H_out, S_out\n",
    "\n",
    "\n",
    "class FusionGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Gated fusion of HGT and LightGCN representations.\n",
    "    \n",
    "    This module learns to adaptively combine the semantic features from HGT\n",
    "    with the collaborative filtering signals from LightGCN using a learned\n",
    "    gating mechanism.\n",
    "    \n",
    "    For each node, a gate value g ∈ [0,1] is computed:\n",
    "        output = g * HGT_emb + (1 - g) * LightGCN_emb\n",
    "    \n",
    "    This allows the model to balance between:\n",
    "    - Content-based matching (HGT: demographic/location features)\n",
    "    - Collaborative filtering (LightGCN: graph structure patterns)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Embedding dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        \"\"\"Initialize fusion gates for households and schools.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Gate for household embeddings\n",
    "        self.gate_h = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim // 2),  # Concatenate both embeddings\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim // 2, 1)          # Single gate value per node\n",
    "        )\n",
    "        \n",
    "        # Gate for school embeddings\n",
    "        self.gate_s = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        hgt_h: torch.Tensor, \n",
    "        hgt_s: torch.Tensor, \n",
    "        lgcn_h: torch.Tensor, \n",
    "        lgcn_s: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute gated fusion of HGT and LightGCN embeddings.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hgt_h, hgt_s : torch.Tensor\n",
    "            HGT embeddings for households and schools.\n",
    "            \n",
    "        lgcn_h, lgcn_s : torch.Tensor\n",
    "            LightGCN embeddings for households and schools.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor]\n",
    "            Fused (household_emb, school_emb).\n",
    "        \"\"\"\n",
    "        # Compute household gates\n",
    "        gh = torch.sigmoid(\n",
    "            self.gate_h(torch.cat([hgt_h, lgcn_h], dim=-1))\n",
    "        )\n",
    "        \n",
    "        # Compute school gates\n",
    "        gs = torch.sigmoid(\n",
    "            self.gate_s(torch.cat([hgt_s, lgcn_s], dim=-1))\n",
    "        )\n",
    "        \n",
    "        # Apply gated fusion\n",
    "        out_h = gh * hgt_h + (1 - gh) * lgcn_h\n",
    "        out_s = gs * hgt_s + (1 - gs) * lgcn_s\n",
    "        \n",
    "        return out_h, out_s\n",
    "\n",
    "\n",
    "class BackboneModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete backbone model combining all components.\n",
    "    \n",
    "    Architecture:\n",
    "        1. FeatureEncoder: Raw features → hidden representations\n",
    "        2. HGTBackbone: Multi-hop heterogeneous message passing\n",
    "        3. LightGCNResidual: Collaborative filtering on bipartite graph\n",
    "        4. FusionGate: Adaptive combination of HGT and LightGCN\n",
    "    \n",
    "    This unified model learns both semantic node representations and\n",
    "    collaborative filtering patterns, fusing them for link prediction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    metadata : tuple\n",
    "        Graph metadata from HeteroData.\n",
    "        \n",
    "    sizes : dict\n",
    "        Categorical vocabulary sizes.\n",
    "        \n",
    "    hidden : int, optional (default=192)\n",
    "        Hidden dimension for feature encoding.\n",
    "        \n",
    "    out_dim : int, optional (default=192)\n",
    "        Output embedding dimension.\n",
    "        \n",
    "    hgt_layers : int, optional (default=3)\n",
    "        Number of HGT layers.\n",
    "        \n",
    "    hgt_heads : int, optional (default=4)\n",
    "        Number of attention heads in HGT.\n",
    "        \n",
    "    lgcn_layers : int, optional (default=2)\n",
    "        Number of LightGCN iterations.\n",
    "        \n",
    "    n_freq : int, optional (default=6)\n",
    "        Fourier frequency bands.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        metadata: tuple, \n",
    "        sizes: dict, \n",
    "        hidden: int = 192, \n",
    "        out_dim: int = 192, \n",
    "        hgt_layers: int = 3, \n",
    "        hgt_heads: int = 4, \n",
    "        lgcn_layers: int = 2, \n",
    "        n_freq: int = 6\n",
    "    ):\n",
    "        \"\"\"Initialize all model components.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature encoding module\n",
    "        self.encoder = FeatureEncoder(sizes, hidden=hidden, n_freq=n_freq)\n",
    "        \n",
    "        # Heterogeneous Graph Transformer\n",
    "        self.hgt = HGTBackbone(\n",
    "            metadata, hidden=hidden, out_dim=out_dim, \n",
    "            layers=hgt_layers, heads=hgt_heads\n",
    "        )\n",
    "        \n",
    "        # LightGCN collaborative filtering\n",
    "        self.lgcn = LightGCNResidual(dim=out_dim, layers=lgcn_layers)\n",
    "        \n",
    "        # Fusion gate\n",
    "        self.fuse = FusionGate(dim=out_dim)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        data: HeteroData, \n",
    "        att_edge_index: torch.Tensor\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Forward pass through the complete backbone.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : HeteroData\n",
    "            Input graph with node features.\n",
    "            \n",
    "        att_edge_index : torch.Tensor\n",
    "            Edge index for 'attends' edges (for LightGCN).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            Final node embeddings {'household': Tensor, 'school': Tensor}.\n",
    "        \"\"\"\n",
    "        # Step 1: Encode raw features\n",
    "        x0 = self.encoder(data)\n",
    "        \n",
    "        # Step 2: Apply HGT\n",
    "        x_hgt = self.hgt(x0, data.edge_index_dict)\n",
    "        \n",
    "        # Step 3: Apply LightGCN if attends edges exist\n",
    "        if ('household', 'attends', 'school') in data.edge_types:\n",
    "            # Use provided att_edge_index or fall back to data\n",
    "            ei = (\n",
    "                att_edge_index if att_edge_index is not None \n",
    "                else data[('household', 'attends', 'school')].edge_index\n",
    "            )\n",
    "            H_lg, S_lg = self.lgcn(x_hgt['household'], x_hgt['school'], ei)\n",
    "        else:\n",
    "            # No LightGCN if no attends edges\n",
    "            H_lg, S_lg = x_hgt['household'], x_hgt['school']\n",
    "        \n",
    "        # Step 4: Fuse HGT and LightGCN embeddings\n",
    "        H, S = self.fuse(x_hgt['household'], x_hgt['school'], H_lg, S_lg)\n",
    "        \n",
    "        return {'household': H, 'school': S}\n",
    "\n",
    "\n",
    "class LinkPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP-based link predictor for scoring (household, school) pairs.\n",
    "    \n",
    "    Given household embedding h and school embedding s, computes:\n",
    "        input = [h, s, |h-s|, h*s]\n",
    "        score = MLP(input)\n",
    "    \n",
    "    The concatenation of multiple interaction features (difference, product)\n",
    "    allows the model to capture various similarity/compatibility patterns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Input embedding dimension.\n",
    "        \n",
    "    hidden : int, optional (default=256)\n",
    "        Hidden layer dimension.\n",
    "        \n",
    "    dropout : float, optional (default=0.2)\n",
    "        Dropout probability for regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, hidden: int = 256, dropout: float = 0.2):\n",
    "        \"\"\"Initialize the link prediction MLP.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input: h (dim) + s (dim) + |h-s| (dim) + h*s (dim) = 4*dim\n",
    "        in_dim = dim * 4\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)  # Single logit output\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        H: torch.Tensor, \n",
    "        S: torch.Tensor, \n",
    "        edge_index: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute link scores for given edges.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        H : torch.Tensor\n",
    "            Household embeddings [N_h, dim].\n",
    "            \n",
    "        S : torch.Tensor\n",
    "            School embeddings [N_s, dim].\n",
    "            \n",
    "        edge_index : torch.Tensor\n",
    "            Edges to score [2, E] where row 0 = households, row 1 = schools.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Link scores (logits) [E].\n",
    "        \"\"\"\n",
    "        # Get embeddings for edge endpoints\n",
    "        h = H[edge_index[0]]  # Household embeddings [E, dim]\n",
    "        s = S[edge_index[1]]  # School embeddings [E, dim]\n",
    "        \n",
    "        # Compute interaction features\n",
    "        x = torch.cat([\n",
    "            h,                    # Household features\n",
    "            s,                    # School features\n",
    "            torch.abs(h - s),     # Absolute difference\n",
    "            h * s                 # Element-wise product\n",
    "        ], dim=-1)  # [E, 4*dim]\n",
    "        \n",
    "        # Predict and flatten\n",
    "        return self.mlp(x).view(-1)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 8: SELF-SUPERVISED PRE-TRAINING\n",
    "# ==============================================================================\n",
    "\n",
    "def mask_edges(data: HeteroData, p: float = 0.5) -> Tuple[HeteroData, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Randomly mask (drop) edges for denoising pre-training.\n",
    "    \n",
    "    Creates a corrupted graph by dropping edges with probability p.\n",
    "    The model learns to reconstruct the original edges from the corrupted graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : HeteroData\n",
    "        Input graph with 'attends' edges.\n",
    "        \n",
    "    p : float, optional (default=0.5)\n",
    "        Probability of dropping each edge.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[HeteroData, torch.Tensor]\n",
    "        - data_corr: Corrupted graph with edges dropped\n",
    "        - original_edges: Original edge index (for reconstruction target)\n",
    "    \"\"\"\n",
    "    key = ('household', 'attends', 'school')\n",
    "    rev = ('school', 'rev_attends', 'household')\n",
    "    \n",
    "    # Check if target edges exist\n",
    "    if key not in data.edge_types:\n",
    "        return data, None\n",
    "    \n",
    "    # Get original edge index\n",
    "    ei = data[key].edge_index\n",
    "    num_e = ei.size(1)\n",
    "    \n",
    "    # Create mask: keep edges with probability (1-p)\n",
    "    keep = (torch.rand(num_e, device=ei.device) > p)\n",
    "    \n",
    "    # Create corrupted graph\n",
    "    data_corr = sanitize_for_resplit(data)\n",
    "    data_corr[key].edge_index = ei[:, keep]\n",
    "    \n",
    "    # Also mask reverse edges\n",
    "    if rev in data_corr.edge_types:\n",
    "        data_corr[rev].edge_index = ei.flip(0)[:, keep]\n",
    "    \n",
    "    return data_corr, ei\n",
    "\n",
    "\n",
    "def sample_neg_bipartite(\n",
    "    num_h: int, \n",
    "    num_s: int, \n",
    "    num_samples: int, \n",
    "    device: torch.device\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample random negative edges for bipartite graph.\n",
    "    \n",
    "    Generates random (household, school) pairs that serve as negative\n",
    "    examples during training. Note: May include actual edges (false negatives)\n",
    "    but this is typically acceptable for large graphs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_h : int\n",
    "        Number of households.\n",
    "        \n",
    "    num_s : int\n",
    "        Number of schools.\n",
    "        \n",
    "    num_samples : int\n",
    "        Number of negative samples to generate.\n",
    "        \n",
    "    device : torch.device\n",
    "        Device for tensor creation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Negative edge index [2, num_samples].\n",
    "    \"\"\"\n",
    "    # Random household indices\n",
    "    h = torch.randint(0, num_h, (num_samples,), device=device)\n",
    "    \n",
    "    # Random school indices\n",
    "    s = torch.randint(0, num_s, (num_samples,), device=device)\n",
    "    \n",
    "    return torch.stack([h, s], dim=0)\n",
    "\n",
    "\n",
    "def info_nce(\n",
    "    H: torch.Tensor, \n",
    "    S: torch.Tensor, \n",
    "    pos_pairs: torch.Tensor, \n",
    "    temp: float = 0.2, \n",
    "    neg_k: int = 64\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute InfoNCE contrastive loss for positive pairs.\n",
    "    \n",
    "    InfoNCE encourages positive pairs to have high similarity while\n",
    "    pushing apart randomly sampled negatives. This is a key component\n",
    "    of self-supervised learning.\n",
    "    \n",
    "    Loss = -log(exp(sim(h,s+)/τ) / Σ exp(sim(h,s)/τ))\n",
    "    \n",
    "    Reference:\n",
    "        Oord et al. (2018) \"Representation Learning with Contrastive\n",
    "        Predictive Coding\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    H : torch.Tensor\n",
    "        Household embeddings [N_h, dim].\n",
    "        \n",
    "    S : torch.Tensor\n",
    "        School embeddings [N_s, dim].\n",
    "        \n",
    "    pos_pairs : torch.Tensor\n",
    "        Positive edge index [2, P].\n",
    "        \n",
    "    temp : float, optional (default=0.2)\n",
    "        Temperature parameter (lower = harder negative mining).\n",
    "        \n",
    "    neg_k : int, optional (default=64)\n",
    "        Number of negative samples per positive.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Scalar InfoNCE loss.\n",
    "    \"\"\"\n",
    "    # Handle empty positive pairs\n",
    "    if pos_pairs is None or pos_pairs.numel() == 0:\n",
    "        return H.new_tensor(0.0)\n",
    "    \n",
    "    # Get positive pair embeddings\n",
    "    h = H[pos_pairs[0]]      # [P, dim]\n",
    "    s_pos = S[pos_pairs[1]]  # [P, dim]\n",
    "    \n",
    "    # Compute positive similarity (cosine)\n",
    "    pos_sim = F.cosine_similarity(h, s_pos, dim=-1).unsqueeze(-1)  # [P, 1]\n",
    "    \n",
    "    # Sample random negative schools for each positive\n",
    "    B = h.size(0)\n",
    "    s_neg_idx = torch.randint(0, S.size(0), (B, neg_k), device=H.device)\n",
    "    s_neg = S[s_neg_idx]  # [P, neg_k, dim]\n",
    "    \n",
    "    # Expand household embeddings for broadcasting\n",
    "    h_rep = h.unsqueeze(1).expand_as(s_neg)  # [P, neg_k, dim]\n",
    "    \n",
    "    # Compute negative similarities\n",
    "    neg_sim = F.cosine_similarity(h_rep, s_neg, dim=-1)  # [P, neg_k]\n",
    "    \n",
    "    # Concatenate positive and negative similarities\n",
    "    logits = torch.cat([pos_sim, neg_sim], dim=1) / temp  # [P, 1+neg_k]\n",
    "    \n",
    "    # Labels: positive is always at index 0\n",
    "    labels = torch.zeros(B, dtype=torch.long, device=H.device)\n",
    "    \n",
    "    # Cross-entropy loss\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "\n",
    "def pretrain_epoch(\n",
    "    model: BackboneModel, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    data: HeteroData, \n",
    "    p_drop: float = 0.5, \n",
    "    lambda_nce: float = 0.1\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Perform one pre-training epoch with denoising reconstruction + InfoNCE.\n",
    "    \n",
    "    Pre-training objectives:\n",
    "    1. Edge reconstruction: Predict masked edges from corrupted graph\n",
    "    2. Spatial reconstruction: Predict spatial proximity edges\n",
    "    3. InfoNCE: Contrastive learning on positive pairs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BackboneModel\n",
    "        The backbone model to train.\n",
    "        \n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for parameter updates.\n",
    "        \n",
    "    data : HeteroData\n",
    "        Training graph (will be corrupted during training).\n",
    "        \n",
    "    p_drop : float, optional (default=0.5)\n",
    "        Edge drop probability for corruption.\n",
    "        \n",
    "    lambda_nce : float, optional (default=0.1)\n",
    "        Weight for InfoNCE loss term.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of loss values for logging.\n",
    "    \"\"\"\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Create corrupted graph\n",
    "    data_corr, pos_att = mask_edges(data, p=p_drop)\n",
    "    \n",
    "    # Get edge index for LightGCN\n",
    "    att_key = ('household', 'attends', 'school')\n",
    "    att_ei = (\n",
    "        data_corr[att_key].edge_index if att_key in data_corr.edge_types \n",
    "        else None\n",
    "    )\n",
    "    \n",
    "    # Forward pass through model\n",
    "    z = model(\n",
    "        data_corr, \n",
    "        att_edge_index=att_ei if att_ei is not None else (\n",
    "            data[att_key].edge_index if att_key in data.edge_types else None\n",
    "        )\n",
    "    )\n",
    "\n",
    "    losses = {}\n",
    "    \n",
    "    # ----- Loss 1: Reconstruct attends edges -----\n",
    "    if pos_att is not None:\n",
    "        num_pos = pos_att.size(1)\n",
    "        \n",
    "        # Sample negative edges\n",
    "        neg_att = sample_neg_bipartite(\n",
    "            data['household'].num_nodes, \n",
    "            data['school'].num_nodes, \n",
    "            num_pos, \n",
    "            device=pos_att.device\n",
    "        )\n",
    "        \n",
    "        # Combine positive and negative edges\n",
    "        all_e = torch.cat([pos_att, neg_att], dim=1)\n",
    "        \n",
    "        # Get embeddings and compute dot product scores\n",
    "        h = z['household'][all_e[0]]\n",
    "        s = z['school'][all_e[1]]\n",
    "        logits = (h * s).sum(dim=-1)\n",
    "        \n",
    "        # Binary labels\n",
    "        y = torch.cat([\n",
    "            torch.ones(num_pos, device=logits.device),\n",
    "            torch.zeros(num_pos, device=logits.device)\n",
    "        ], dim=0)\n",
    "        \n",
    "        # BCE loss\n",
    "        losses['recon_att'] = F.binary_cross_entropy_with_logits(logits, y)\n",
    "\n",
    "    # ----- Loss 2: Reconstruct household spatial edges -----\n",
    "    if ('household', 'spatially_near', 'household') in data.edge_types:\n",
    "        e = data[('household', 'spatially_near', 'household')].edge_index\n",
    "        logits = (z['household'][e[0]] * z['household'][e[1]]).sum(dim=-1)\n",
    "        y = torch.ones(logits.size(0), device=logits.device)\n",
    "        losses['recon_hh'] = F.binary_cross_entropy_with_logits(logits, y)\n",
    "\n",
    "    # ----- Loss 3: Reconstruct school spatial edges -----\n",
    "    if ('school', 'near', 'school') in data.edge_types:\n",
    "        e = data[('school', 'near', 'school')].edge_index\n",
    "        logits = (z['school'][e[0]] * z['school'][e[1]]).sum(dim=-1)\n",
    "        y = torch.ones(logits.size(0), device=logits.device)\n",
    "        losses['recon_ss'] = F.binary_cross_entropy_with_logits(logits, y)\n",
    "\n",
    "    # ----- Loss 4: InfoNCE contrastive loss -----\n",
    "    if pos_att is not None:\n",
    "        losses['info_nce'] = info_nce(\n",
    "            z['household'], z['school'], pos_att, temp=0.2, neg_k=64\n",
    "        )\n",
    "\n",
    "    # Combine losses with weights\n",
    "    loss = (\n",
    "        losses.get('recon_att', 0.0) + \n",
    "        0.5 * losses.get('recon_hh', 0.0) + \n",
    "        0.5 * losses.get('recon_ss', 0.0) + \n",
    "        lambda_nce * losses.get('info_nce', 0.0)\n",
    "    )\n",
    "    \n",
    "    # Ensure loss is a tensor\n",
    "    if not torch.is_tensor(loss):\n",
    "        loss = torch.tensor(loss, device=z['household'].device)\n",
    "    \n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return losses as floats for logging\n",
    "    return {k: float(v) if torch.is_tensor(v) else v for k, v in losses.items()}\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 9: EMBEDDING COMPUTATION AND CANDIDATE POOLS\n",
    "# ==============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_embeddings(\n",
    "    model: BackboneModel, \n",
    "    split_graph: HeteroData, \n",
    "    device: torch.device\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute frozen embeddings for all nodes.\n",
    "    \n",
    "    This function generates embeddings without gradient computation,\n",
    "    useful for evaluation and frozen-backbone fine-tuning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : BackboneModel\n",
    "        Trained backbone model.\n",
    "        \n",
    "    split_graph : HeteroData\n",
    "        Graph to compute embeddings on.\n",
    "        \n",
    "    device : torch.device\n",
    "        Device for computation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        CPU tensors {'household': [N_h, dim], 'school': [N_s, dim]}.\n",
    "    \"\"\"\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move graph to device\n",
    "    split_graph = split_graph.to(device)\n",
    "    \n",
    "    # Get attends edge index\n",
    "    att_ei = split_graph[('household', 'attends', 'school')].edge_index\n",
    "    \n",
    "    # Forward pass without gradients\n",
    "    z = model(split_graph, att_edge_index=att_ei)\n",
    "    \n",
    "    # Return detached CPU tensors\n",
    "    return {k: v.detach().cpu() for k, v in z.items()}\n",
    "\n",
    "\n",
    "def build_geo_pools(\n",
    "    house_xy: torch.Tensor, \n",
    "    school_xy: torch.Tensor, \n",
    "    k_geo: int = 50\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build geographic candidate pools for hard negative sampling.\n",
    "    \n",
    "    For each household, find the k geographically closest schools.\n",
    "    These form a pool of plausible but likely non-linked schools,\n",
    "    making them effective hard negatives.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    house_xy : torch.Tensor\n",
    "        Household coordinates [N_h, 2].\n",
    "        \n",
    "    school_xy : torch.Tensor\n",
    "        School coordinates [N_s, 2].\n",
    "        \n",
    "    k_geo : int, optional (default=50)\n",
    "        Number of nearest schools per household.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Nearest school indices [N_h, k_geo].\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Compute pairwise Euclidean distances\n",
    "        d2 = torch.cdist(house_xy.float(), school_xy.float(), p=2)\n",
    "        \n",
    "        # Find k nearest (using negative distance for top-k)\n",
    "        idx = torch.topk(-d2, k=min(k_geo, school_xy.size(0)), dim=1).indices\n",
    "        \n",
    "        return idx  # [N_h, k]\n",
    "\n",
    "\n",
    "def build_emb_pools(\n",
    "    H: torch.Tensor, \n",
    "    S: torch.Tensor, \n",
    "    k_emb: int = 50, \n",
    "    batch: int = 1024\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build embedding-based candidate pools for hard negative sampling.\n",
    "    \n",
    "    For each household, find the k most similar schools by embedding\n",
    "    cosine similarity. These are semantically plausible candidates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    H : torch.Tensor\n",
    "        Household embeddings [N_h, dim].\n",
    "        \n",
    "    S : torch.Tensor\n",
    "        School embeddings [N_s, dim].\n",
    "        \n",
    "    k_emb : int, optional (default=50)\n",
    "        Number of similar schools per household.\n",
    "        \n",
    "    batch : int, optional (default=1024)\n",
    "        Batch size for memory efficiency.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Similar school indices [N_h, k_emb].\n",
    "    \"\"\"\n",
    "    # Normalize embeddings for cosine similarity\n",
    "    Hn = F.normalize(H, dim=-1)\n",
    "    Sn = F.normalize(S, dim=-1)\n",
    "    \n",
    "    all_idx = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process in batches to avoid OOM\n",
    "        for i in range(0, Hn.size(0), batch):\n",
    "            h = Hn[i:i + batch]\n",
    "            \n",
    "            # Compute cosine similarities\n",
    "            sim = h @ Sn.t()\n",
    "            \n",
    "            # Find top-k similar schools\n",
    "            idx = torch.topk(sim, k=min(k_emb, Sn.size(0)), dim=1).indices\n",
    "            all_idx.append(idx.cpu())\n",
    "    \n",
    "    return torch.cat(all_idx, dim=0)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 10: ROBUST EVALUATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def _build_eval_pairs(split: HeteroData) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Build evaluation pairs (edges + labels) from a split.\n",
    "    \n",
    "    Handles both cases:\n",
    "    1. Split has edge_label_index and edge_label (from RandomLinkSplit)\n",
    "    2. Fallback: use edge_index as positives, sample random negatives\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    split : HeteroData\n",
    "        Graph split with edge information.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor]\n",
    "        - edge_label_index: Edges to evaluate [2, E]\n",
    "        - labels: Binary labels [E] (1.0 = positive, 0.0 = negative)\n",
    "    \"\"\"\n",
    "    key = ('household', 'attends', 'school')\n",
    "    E = split[key]\n",
    "    Hn = split['household'].num_nodes\n",
    "    Sn = split['school'].num_nodes\n",
    "\n",
    "    # Check if split has proper labels\n",
    "    if hasattr(E, 'edge_label_index') and hasattr(E, 'edge_label'):\n",
    "        return E.edge_label_index, E.edge_label.float()\n",
    "\n",
    "    # Fallback: positives = edge_index; negatives = random non-edges\n",
    "    pos = E.edge_index\n",
    "    num_pos = pos.size(1)\n",
    "    \n",
    "    # Create set of positive edges for fast lookup\n",
    "    pos_set = set((int(pos[0, i]), int(pos[1, i])) for i in range(num_pos))\n",
    "    \n",
    "    # Sample negative edges\n",
    "    neg = []\n",
    "    need = num_pos\n",
    "    rng = np.random.default_rng(12345)\n",
    "    \n",
    "    while len(neg) < need:\n",
    "        h = int(rng.integers(0, Hn))\n",
    "        s = int(rng.integers(0, Sn))\n",
    "        if (h, s) not in pos_set:\n",
    "            neg.append((h, s))\n",
    "    \n",
    "    neg = torch.tensor(neg, dtype=torch.long).t()\n",
    "    \n",
    "    # Combine positive and negative edges\n",
    "    edge_lab_idx = torch.cat([pos, neg], dim=1)\n",
    "    labels = torch.cat([\n",
    "        torch.ones(num_pos), \n",
    "        torch.zeros(need)\n",
    "    ]).float()\n",
    "    \n",
    "    return edge_lab_idx, labels\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_logits_labels(\n",
    "    predictor: LinkPredictor, \n",
    "    z: dict, \n",
    "    split: HeteroData\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Get model predictions and labels for a split.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictor : LinkPredictor\n",
    "        Trained link predictor.\n",
    "        \n",
    "    z : dict\n",
    "        Pre-computed embeddings.\n",
    "        \n",
    "    split : HeteroData\n",
    "        Graph split to evaluate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[torch.Tensor, torch.Tensor]\n",
    "        - logits: Model scores [E]\n",
    "        - labels: Ground truth [E]\n",
    "    \"\"\"\n",
    "    edge_label_index, edge_label = _build_eval_pairs(split)\n",
    "    logits = predictor(z['household'], z['school'], edge_label_index)\n",
    "    return logits, edge_label\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_edge_label(\n",
    "    predictor: LinkPredictor, \n",
    "    z: dict, \n",
    "    split: HeteroData, \n",
    "    report_calibration: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate link prediction performance with multiple metrics.\n",
    "    \n",
    "    Metrics computed:\n",
    "    - AUC-ROC: Discrimination ability\n",
    "    - Average Precision: Performance on positive class\n",
    "    - Brier Score: Calibration + refinement\n",
    "    - ECE: Expected Calibration Error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictor : LinkPredictor\n",
    "        Trained link predictor.\n",
    "        \n",
    "    z : dict\n",
    "        Pre-computed embeddings.\n",
    "        \n",
    "    split : HeteroData\n",
    "        Graph split to evaluate.\n",
    "        \n",
    "    report_calibration : bool, optional (default=True)\n",
    "        Whether to compute calibration metrics.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of metric values.\n",
    "    \"\"\"\n",
    "    predictor.eval()\n",
    "    \n",
    "    # Get predictions and labels\n",
    "    logits, y = get_logits_labels(predictor, z, split)\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "    \n",
    "    # Compute discrimination metrics\n",
    "    auc = roc_auc_score(y_np, probs)\n",
    "    ap = average_precision_score(y_np, probs)\n",
    "    \n",
    "    out = {'auc': float(auc), 'ap': float(ap)}\n",
    "    \n",
    "    # Compute calibration metrics\n",
    "    if report_calibration:\n",
    "        out['brier'] = float(brier_score_loss(y_np, probs))\n",
    "        \n",
    "        # Expected Calibration Error\n",
    "        n_bins = 15\n",
    "        bins = np.linspace(0, 1, n_bins + 1)\n",
    "        inds = np.digitize(probs, bins) - 1\n",
    "        ece = 0.0\n",
    "        \n",
    "        for b in range(n_bins):\n",
    "            mask = inds == b\n",
    "            if mask.any():\n",
    "                conf = probs[mask].mean()\n",
    "                acc = y_np[mask].mean()\n",
    "                ece += abs(acc - conf) * (mask.sum() / len(probs))\n",
    "        \n",
    "        out['ece'] = float(ece)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_ranking_candidates(\n",
    "    predictor: LinkPredictor, \n",
    "    z: dict, \n",
    "    split: HeteroData, \n",
    "    ks: tuple = (1, 3, 5, 10), \n",
    "    cand_geo: int = 50, \n",
    "    cand_rand: int = 50\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate ranking performance with Hit@k and NDCG@k.\n",
    "    \n",
    "    For each household with positive edges, creates a candidate set of\n",
    "    schools and ranks them by model score. Evaluates how often the true\n",
    "    positive appears in top-k.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictor : LinkPredictor\n",
    "        Trained link predictor.\n",
    "        \n",
    "    z : dict\n",
    "        Pre-computed embeddings.\n",
    "        \n",
    "    split : HeteroData\n",
    "        Graph split to evaluate.\n",
    "        \n",
    "    ks : tuple, optional (default=(1,3,5,10))\n",
    "        k values for Hit@k and NDCG@k.\n",
    "        \n",
    "    cand_geo : int, optional (default=50)\n",
    "        Number of similar candidates (by embedding).\n",
    "        \n",
    "    cand_rand : int, optional (default=50)\n",
    "        Number of random candidates.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary of ranking metrics.\n",
    "    \"\"\"\n",
    "    predictor.eval()\n",
    "    \n",
    "    # Get evaluation pairs\n",
    "    eidx, labs = _build_eval_pairs(split)\n",
    "    hs_unique = eidx[0].unique().cpu()\n",
    "    \n",
    "    S = z['school']\n",
    "    H = z['household']\n",
    "    S_all = S.size(0)\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    # Initialize result accumulators\n",
    "    results = {f'hit@{k}': 0.0 for k in ks}\n",
    "    results.update({f'ndcg@{k}': 0.0 for k in ks})\n",
    "    count = 0\n",
    "\n",
    "    # Build positive edge map: household -> set of true positive schools\n",
    "    pos_map = {}\n",
    "    for i in range(eidx.size(1)):\n",
    "        h, s = int(eidx[0, i]), int(eidx[1, i])\n",
    "        if labs[i].item() == 1.0:\n",
    "            pos_map.setdefault(h, set()).add(s)\n",
    "\n",
    "    # Evaluate each household\n",
    "    for h in hs_unique:\n",
    "        h = int(h.item())\n",
    "        \n",
    "        # Skip households without positive edges\n",
    "        if h not in pos_map or len(pos_map[h]) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Build candidate set: similar by embedding + random\n",
    "        with torch.no_grad():\n",
    "            sim = F.normalize(H[h:h + 1], dim=-1) @ F.normalize(S, dim=-1).t()\n",
    "            top_sim = torch.topk(sim.squeeze(0), k=min(cand_geo, S_all)).indices.cpu()\n",
    "        \n",
    "        rand_cand = torch.tensor(\n",
    "            rng.choice(S_all, size=min(cand_rand, S_all), replace=False)\n",
    "        )\n",
    "        cand = torch.unique(torch.cat([top_sim, rand_cand], dim=0)).numpy()\n",
    "\n",
    "        # Score candidates\n",
    "        scores = predictor(\n",
    "            H, S, \n",
    "            torch.stack([\n",
    "                torch.tensor([h]).repeat(len(cand)), \n",
    "                torch.tensor(cand)\n",
    "            ], dim=0)\n",
    "        ).sigmoid().cpu().numpy()\n",
    "        \n",
    "        # Rank by score (descending)\n",
    "        order = np.argsort(-scores)\n",
    "        ranked = cand[order]\n",
    "\n",
    "        # Compute metrics for each k\n",
    "        for k in ks:\n",
    "            topk = ranked[:min(k, ranked.shape[0])]\n",
    "            \n",
    "            # Hit@k: 1 if any true positive in top-k\n",
    "            hit = 1.0 if any((s in pos_map[h]) for s in topk) else 0.0\n",
    "            \n",
    "            # NDCG@k\n",
    "            rel = np.array([\n",
    "                1.0 if (t in pos_map[h]) else 0.0 for t in topk\n",
    "            ], dtype=float)\n",
    "            \n",
    "            if rel.size > 0:\n",
    "                dcg = np.sum(rel / np.log2(np.arange(2, 2 + rel.size)))\n",
    "                idcg = 1.0  # Only one positive per query assumed\n",
    "                ndcg = dcg / idcg\n",
    "            else:\n",
    "                ndcg = 0.0\n",
    "            \n",
    "            results[f'hit@{k}'] += hit\n",
    "            results[f'ndcg@{k}'] += ndcg\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "    # Average over households\n",
    "    if count > 0:\n",
    "        for k in ks:\n",
    "            results[f'hit@{k}'] /= count\n",
    "            results[f'ndcg@{k}'] /= count\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 11: FINE-TUNING WITH HARD NEGATIVES\n",
    "# ==============================================================================\n",
    "\n",
    "def get_pos_edges(split: HeteroData) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract positive edges from a split.\n",
    "    \n",
    "    Handles both labeled splits (with edge_label) and raw edge_index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    split : HeteroData\n",
    "        Graph split.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Positive edge index [2, num_pos].\n",
    "    \"\"\"\n",
    "    key = ('household', 'attends', 'school')\n",
    "    \n",
    "    if hasattr(split[key], 'edge_label') and hasattr(split[key], 'edge_label_index'):\n",
    "        eidx = split[key].edge_label_index\n",
    "        lab = split[key].edge_label\n",
    "        pos = eidx[:, lab == 1]\n",
    "        return pos\n",
    "    else:\n",
    "        return split[key].edge_index\n",
    "\n",
    "\n",
    "def sample_hard_negatives(\n",
    "    h_nodes: torch.Tensor, \n",
    "    pools_geo: torch.Tensor, \n",
    "    pools_emb: torch.Tensor, \n",
    "    num_neg: int = 3\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample hard negative schools from geographic and embedding pools.\n",
    "    \n",
    "    For each household, samples from the union of:\n",
    "    - Geographically close schools\n",
    "    - Semantically similar schools (by embedding)\n",
    "    \n",
    "    These are \"hard\" negatives because they are plausible but incorrect.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    h_nodes : torch.Tensor\n",
    "        Household indices [B].\n",
    "        \n",
    "    pools_geo : torch.Tensor\n",
    "        Geographic candidate pools [N_h, k_geo].\n",
    "        \n",
    "    pools_emb : torch.Tensor\n",
    "        Embedding candidate pools [N_h, k_emb].\n",
    "        \n",
    "    num_neg : int, optional (default=3)\n",
    "        Number of negatives per household.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Negative school indices [B, num_neg].\n",
    "    \"\"\"\n",
    "    B = h_nodes.size(0)\n",
    "    out = []\n",
    "    \n",
    "    for i in range(B):\n",
    "        h = int(h_nodes[i].item())\n",
    "        \n",
    "        # Combine geographic and embedding pools\n",
    "        cand = torch.unique(torch.cat([pools_geo[h], pools_emb[h]], dim=0))\n",
    "        \n",
    "        if cand.numel() == 0:\n",
    "            out.append(torch.empty(0, dtype=torch.long))\n",
    "        else:\n",
    "            # Sample from candidates (with replacement if needed)\n",
    "            if cand.numel() < num_neg:\n",
    "                idx = torch.randint(0, cand.numel(), (num_neg,))\n",
    "                out.append(cand[idx])\n",
    "            else:\n",
    "                perm = torch.randperm(cand.numel())\n",
    "                out.append(cand[perm[:num_neg]])\n",
    "    \n",
    "    return torch.stack(out, dim=0)  # [B, num_neg]\n",
    "\n",
    "\n",
    "def bce_weight(pos: int, neg: int) -> float:\n",
    "    \"\"\"\n",
    "    Compute positive weight for imbalanced BCE loss.\n",
    "    \n",
    "    Compensates for class imbalance by weighting positive samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos : int\n",
    "        Number of positive samples.\n",
    "        \n",
    "    neg : int\n",
    "        Number of negative samples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Weight for positive class.\n",
    "    \"\"\"\n",
    "    if pos == 0:\n",
    "        return 1.0\n",
    "    return float(neg / pos)\n",
    "\n",
    "\n",
    "def finetune_epoch_frozen(\n",
    "    predictor: LinkPredictor, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    train_split: HeteroData, \n",
    "    z_train: dict, \n",
    "    pools_geo: torch.Tensor, \n",
    "    pools_emb: torch.Tensor, \n",
    "    cfg: dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Fine-tune link predictor with frozen backbone embeddings.\n",
    "    \n",
    "    Uses hard negative sampling from geographic and embedding pools\n",
    "    to train a discriminative link predictor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    predictor : LinkPredictor\n",
    "        Link predictor to train.\n",
    "        \n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for predictor parameters.\n",
    "        \n",
    "    train_split : HeteroData\n",
    "        Training graph split.\n",
    "        \n",
    "    z_train : dict\n",
    "        Pre-computed embeddings from frozen backbone.\n",
    "        \n",
    "    pools_geo : torch.Tensor\n",
    "        Geographic candidate pools.\n",
    "        \n",
    "    pools_emb : torch.Tensor\n",
    "        Embedding candidate pools.\n",
    "        \n",
    "    cfg : dict\n",
    "        Configuration dictionary with 'FT_BATCH' and 'NEG_PER_POS'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Average training loss.\n",
    "    \"\"\"\n",
    "    predictor.train()\n",
    "    \n",
    "    # Get positive edges\n",
    "    pos = get_pos_edges(train_split)\n",
    "    P = pos.size(1)\n",
    "    bs = cfg['FT_BATCH']\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_pos = 0\n",
    "    \n",
    "    # Iterate over batches\n",
    "    for st in range(0, P, bs):\n",
    "        en = min(st + bs, P)\n",
    "        pos_b = pos[:, st:en]\n",
    "        h_nodes = pos_b[0]\n",
    "        \n",
    "        # Sample hard negatives\n",
    "        neg_per_pos = cfg['NEG_PER_POS']\n",
    "        neg_s = sample_hard_negatives(h_nodes, pools_geo, pools_emb, num_neg=neg_per_pos)\n",
    "        neg_h = h_nodes.unsqueeze(-1).expand_as(neg_s)\n",
    "        neg_edges = torch.stack([neg_h.reshape(-1), neg_s.reshape(-1)], dim=0)\n",
    "\n",
    "        # Combine positive and negative edges\n",
    "        all_edges = torch.cat([pos_b, neg_edges], dim=1)\n",
    "        \n",
    "        # Create labels\n",
    "        y = torch.cat([\n",
    "            torch.ones(pos_b.size(1)), \n",
    "            torch.zeros(neg_edges.size(1))\n",
    "        ], dim=0).to(z_train['household'].device)\n",
    "\n",
    "        # Forward pass and loss\n",
    "        logits = predictor(\n",
    "            z_train['household'], \n",
    "            z_train['school'], \n",
    "            all_edges.to(z_train['household'].device)\n",
    "        )\n",
    "        \n",
    "        pw = bce_weight(pos_b.size(1), neg_edges.size(1))\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits, y, \n",
    "            pos_weight=torch.tensor(pw, device=y.device)\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += float(loss) * pos_b.size(1)\n",
    "        total_pos += pos_b.size(1)\n",
    "    \n",
    "    return total_loss / max(1, total_pos)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 12: DEFAULT CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "DEFAULT_CFG = dict(\n",
    "    # Pre-training hyperparameters\n",
    "    PRE_EPOCHS=80,        # Number of pre-training epochs\n",
    "    PRE_LR=1e-3,          # Pre-training learning rate\n",
    "    PRE_WD=1e-5,          # Pre-training weight decay\n",
    "    PRE_DROP=0.5,         # Edge drop probability for masking\n",
    "    PRE_NCE=0.1,          # InfoNCE loss weight\n",
    "    \n",
    "    # Model architecture\n",
    "    HIDDEN=192,           # Hidden dimension\n",
    "    OUT=192,              # Output embedding dimension\n",
    "    HGT_LAYERS=3,         # Number of HGT layers\n",
    "    HGT_HEADS=4,          # Number of attention heads\n",
    "    LGCN_LAYERS=2,        # Number of LightGCN layers\n",
    "    N_FREQ=6,             # Fourier frequency bands\n",
    "    \n",
    "    # Fine-tuning (frozen backbone)\n",
    "    FT_LR_HEAD=8e-3,      # Learning rate for predictor head\n",
    "    FT_WD_HEAD=1e-4,      # Weight decay for predictor head\n",
    "    FT_EPOCHS_FREEZE=120, # Epochs with frozen backbone\n",
    "    FT_BATCH=4096,        # Batch size for fine-tuning\n",
    "    NEG_PER_POS=3,        # Negatives per positive\n",
    "    \n",
    "    # Fine-tuning (unfrozen backbone)\n",
    "    FT_EPOCHS_UNFREEZE=15,  # Epochs with unfrozen backbone\n",
    "    FT_LR_UNFREEZE=2e-4,    # Learning rate for joint training\n",
    "    FT_WD_UNFREEZE=0.0,     # Weight decay for joint training\n",
    "    \n",
    "    # Evaluation\n",
    "    METRIC_KS=[1, 3, 5, 10],      # k values for ranking metrics\n",
    "    RANK_CAND_GEO=60,             # Geographic candidates for ranking\n",
    "    RANK_CAND_RANDOM=60,          # Random candidates for ranking\n",
    "    REPORT_CALIBRATION=True,      # Whether to compute calibration metrics\n",
    "    \n",
    "    # Training\n",
    "    LOG_INTERVAL=5,               # Log every N epochs\n",
    "    EARLY_STOP_FROZEN=30,         # Early stopping patience (frozen)\n",
    "    EARLY_STOP_UNFREEZE=6         # Early stopping patience (unfrozen)\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 13: ROBUSTNESS TESTING UTILITIES\n",
    "# ==============================================================================\n",
    "\n",
    "def remove_works_at_leak(split_graph: HeteroData) -> HeteroData:\n",
    "    \"\"\"\n",
    "    Remove works_at edges that overlap with positive attends edges.\n",
    "    \n",
    "    This ablation tests whether the model relies on the \"shortcut\" of\n",
    "    staff employment edges that directly reveal school assignments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    split_graph : HeteroData\n",
    "        Graph potentially containing leaking works_at edges.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    HeteroData\n",
    "        Graph with overlapping works_at edges removed.\n",
    "    \"\"\"\n",
    "    key_att = ('household', 'attends', 'school')\n",
    "    key_wrk = ('household', 'works_at', 'school')\n",
    "    rev_wrk = ('school', 'rev_works_at', 'household')\n",
    "    \n",
    "    # Check if works_at edges exist\n",
    "    if key_wrk not in split_graph.edge_types:\n",
    "        return split_graph\n",
    "    \n",
    "    # Get positive attends edges\n",
    "    if hasattr(split_graph[key_att], 'edge_label_index') and hasattr(split_graph[key_att], 'edge_label'):\n",
    "        pos_pairs = split_graph[key_att].edge_label_index[:, split_graph[key_att].edge_label == 1]\n",
    "    else:\n",
    "        pos_pairs = split_graph[key_att].edge_index\n",
    "    \n",
    "    # Create set of positive pairs\n",
    "    pos_set = set(\n",
    "        (int(pos_pairs[0, i]), int(pos_pairs[1, i])) \n",
    "        for i in range(pos_pairs.size(1))\n",
    "    )\n",
    "    \n",
    "    # Filter works_at edges\n",
    "    wrk_ei = split_graph[key_wrk].edge_index\n",
    "    keep = torch.ones(wrk_ei.size(1), dtype=torch.bool)\n",
    "    \n",
    "    for i in range(wrk_ei.size(1)):\n",
    "        if (int(wrk_ei[0, i]), int(wrk_ei[1, i])) in pos_set:\n",
    "            keep[i] = False\n",
    "    \n",
    "    # Create filtered graph\n",
    "    ng = sanitize_for_resplit(split_graph)\n",
    "    ng[key_wrk].edge_index = wrk_ei[:, keep]\n",
    "    \n",
    "    if rev_wrk in ng.edge_types:\n",
    "        ng[rev_wrk].edge_index = ng[key_wrk].edge_index.flip(0)\n",
    "    \n",
    "    return ng\n",
    "\n",
    "\n",
    "def make_household_cold_start_split(\n",
    "    data: HeteroData, \n",
    "    test_frac: float = 0.20, \n",
    "    seed: int = 123\n",
    ") -> Tuple[HeteroData, list, HeteroData]:\n",
    "    \"\"\"\n",
    "    Create inductive split with held-out households (cold-start).\n",
    "    \n",
    "    Tests generalization to completely new households not seen during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : HeteroData\n",
    "        Full graph.\n",
    "        \n",
    "    test_frac : float, optional (default=0.20)\n",
    "        Fraction of households to hold out.\n",
    "        \n",
    "    seed : int, optional (default=123)\n",
    "        Random seed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[HeteroData, list, HeteroData]\n",
    "        - train_graph: Graph without held-out households' edges\n",
    "        - folds: Cross-validation folds on training graph\n",
    "        - test_split: Held-out household edges\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    key = ('household', 'attends', 'school')\n",
    "    H = data['household'].num_nodes\n",
    "    \n",
    "    # Random permutation of households\n",
    "    perm = torch.randperm(H)\n",
    "    n_test = max(1, int(test_frac * H))\n",
    "    held_households = perm[:n_test]\n",
    "    \n",
    "    # Create mask for held-out households\n",
    "    mask_held_h = torch.zeros(H, dtype=torch.bool)\n",
    "    mask_held_h[held_households] = True\n",
    "    \n",
    "    # Identify edges involving held-out households\n",
    "    ei = data[key].edge_index\n",
    "    is_test_edge = mask_held_h[ei[0]]\n",
    "    \n",
    "    def copy_keep(mask):\n",
    "        \"\"\"Helper to create graph with filtered edges.\"\"\"\n",
    "        g = sanitize_for_resplit(data)\n",
    "        g[key].edge_index = ei[:, mask]\n",
    "        rev = ('school', 'rev_attends', 'household')\n",
    "        if rev in g.edge_types:\n",
    "            g[rev].edge_index = g[key].edge_index.flip(0)\n",
    "        return g\n",
    "    \n",
    "    # Create train and test graphs\n",
    "    train_graph = copy_keep(~is_test_edge)\n",
    "    test_split = copy_keep(is_test_edge)\n",
    "    \n",
    "    # Create CV folds on training graph\n",
    "    folds = make_k_dev_folds(train_graph, k=5, base_seed=1000, val_ratio=0.10)\n",
    "    \n",
    "    return train_graph, folds, test_split\n",
    "\n",
    "\n",
    "def make_school_cold_start_split(\n",
    "    data: HeteroData, \n",
    "    test_frac: float = 0.20, \n",
    "    seed: int = 123\n",
    ") -> Tuple[HeteroData, list, HeteroData]:\n",
    "    \"\"\"\n",
    "    Create inductive split with held-out schools (cold-start).\n",
    "    \n",
    "    Tests generalization to completely new schools not seen during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : HeteroData\n",
    "        Full graph.\n",
    "        \n",
    "    test_frac : float, optional (default=0.20)\n",
    "        Fraction of schools to hold out.\n",
    "        \n",
    "    seed : int, optional (default=123)\n",
    "        Random seed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[HeteroData, list, HeteroData]\n",
    "        - train_graph: Graph without held-out schools' edges\n",
    "        - folds: Cross-validation folds on training graph\n",
    "        - test_split: Held-out school edges\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    key = ('household', 'attends', 'school')\n",
    "    S = data['school'].num_nodes\n",
    "    \n",
    "    # Random permutation of schools\n",
    "    perm = torch.randperm(S)\n",
    "    n_test = max(1, int(test_frac * S))\n",
    "    held_schools = perm[:n_test]\n",
    "    \n",
    "    # Create mask for held-out schools\n",
    "    mask_held_s = torch.zeros(S, dtype=torch.bool)\n",
    "    mask_held_s[held_schools] = True\n",
    "    \n",
    "    # Identify edges involving held-out schools\n",
    "    ei = data[key].edge_index\n",
    "    is_test_edge = mask_held_s[ei[1]]\n",
    "    \n",
    "    def copy_keep(mask):\n",
    "        \"\"\"Helper to create graph with filtered edges.\"\"\"\n",
    "        g = sanitize_for_resplit(data)\n",
    "        g[key].edge_index = ei[:, mask]\n",
    "        rev = ('school', 'rev_attends', 'household')\n",
    "        if rev in g.edge_types:\n",
    "            g[rev].edge_index = g[key].edge_index.flip(0)\n",
    "        return g\n",
    "    \n",
    "    # Create train and test graphs\n",
    "    train_graph = copy_keep(~is_test_edge)\n",
    "    test_split = copy_keep(is_test_edge)\n",
    "    \n",
    "    # Create CV folds on training graph\n",
    "    folds = make_k_dev_folds(train_graph, k=5, base_seed=1000, val_ratio=0.10)\n",
    "    \n",
    "    return train_graph, folds, test_split\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 14: TRAINING ORCHESTRATION\n",
    "# ==============================================================================\n",
    "\n",
    "def train_with_splits(\n",
    "    device: torch.device, \n",
    "    data: HeteroData, \n",
    "    sizes: dict, \n",
    "    cfg: dict, \n",
    "    work_dir: str, \n",
    "    seed: int, \n",
    "    dev_graph: HeteroData, \n",
    "    folds: list, \n",
    "    test_split: HeteroData,\n",
    "    do_leakage_ablation: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Complete training pipeline with pre-training, fine-tuning, and evaluation.\n",
    "    \n",
    "    Pipeline stages:\n",
    "    1. Pre-train backbone with denoising + InfoNCE\n",
    "    2. Build candidate pools for hard negative sampling\n",
    "    3. Fine-tune predictor head with frozen backbone\n",
    "    4. (Optional) Joint fine-tuning with unfrozen backbone\n",
    "    5. Evaluate on test set\n",
    "    6. (Optional) Leakage ablation experiment\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "        Computation device.\n",
    "        \n",
    "    data : HeteroData\n",
    "        Full graph (for metadata).\n",
    "        \n",
    "    sizes : dict\n",
    "        Categorical vocabulary sizes.\n",
    "        \n",
    "    cfg : dict\n",
    "        Training configuration.\n",
    "        \n",
    "    work_dir : str\n",
    "        Directory to save results.\n",
    "        \n",
    "    seed : int\n",
    "        Random seed.\n",
    "        \n",
    "    dev_graph : HeteroData\n",
    "        Development graph for training.\n",
    "        \n",
    "    folds : list\n",
    "        Cross-validation folds.\n",
    "        \n",
    "    test_split : HeteroData\n",
    "        Held-out test split.\n",
    "        \n",
    "    do_leakage_ablation : bool, optional (default=True)\n",
    "        Whether to run leakage ablation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Summary of all results.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    set_seed(seed)\n",
    "\n",
    "    # =========================================================================\n",
    "    # STAGE 1: PRE-TRAINING\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Initialize backbone model\n",
    "    model = BackboneModel(\n",
    "        metadata=data.metadata(), \n",
    "        sizes=sizes,\n",
    "        hidden=cfg['HIDDEN'], \n",
    "        out_dim=cfg['OUT'],\n",
    "        hgt_layers=cfg['HGT_LAYERS'], \n",
    "        hgt_heads=cfg['HGT_HEADS'],\n",
    "        lgcn_layers=cfg['LGCN_LAYERS'], \n",
    "        n_freq=cfg['N_FREQ']\n",
    "    ).to(device)\n",
    "    \n",
    "    tick(\"[Robust] Stage 1 pretraining on dev_graph\")\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    opt_pre = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=cfg['PRE_LR'], \n",
    "        weight_decay=cfg['PRE_WD']\n",
    "    )\n",
    "    \n",
    "    # Prepare full graph for pre-training\n",
    "    g_full = sanitize_for_resplit(dev_graph).to(device)\n",
    "    \n",
    "    # Pre-training loop\n",
    "    for ep in range(1, cfg['PRE_EPOCHS'] + 1):\n",
    "        vals = pretrain_epoch(\n",
    "            model, opt_pre, g_full, \n",
    "            p_drop=cfg['PRE_DROP'], \n",
    "            lambda_nce=cfg['PRE_NCE']\n",
    "        )\n",
    "        \n",
    "        # Log progress\n",
    "        if ep % cfg['LOG_INTERVAL'] == 0:\n",
    "            loss_str = \" \".join([\n",
    "                f\"{k}:{vals.get(k, 0):.4f}\" \n",
    "                for k in ['recon_att', 'recon_hh', 'recon_ss', 'info_nce']\n",
    "            ])\n",
    "            print(f\"  [Pretrain {ep:03d}] {loss_str}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # STAGE 2: BUILD CANDIDATE POOLS\n",
    "    # =========================================================================\n",
    "    \n",
    "    tick(\"[Robust] Computing pools on dev_graph\")\n",
    "    \n",
    "    # Geographic coordinates for pool building\n",
    "    house_xy = dev_graph['household'].x[:, -2:].clone()\n",
    "    school_xy = dev_graph['school'].x.clone()\n",
    "    \n",
    "    # Build geographic candidate pool\n",
    "    pools_geo = build_geo_pools(house_xy, school_xy, k_geo=cfg['RANK_CAND_GEO'])\n",
    "    \n",
    "    # Compute embeddings for embedding-based pool\n",
    "    z_dev = compute_embeddings(model, dev_graph, device)\n",
    "    \n",
    "    # Build embedding candidate pool\n",
    "    pools_emb = build_emb_pools(\n",
    "        z_dev['household'], z_dev['school'], \n",
    "        k_emb=cfg['RANK_CAND_RANDOM']\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STAGE 3: FINE-TUNE ON CROSS-VALIDATION FOLDS\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Pre-compute embeddings for all folds\n",
    "    z_folds = []\n",
    "    for tr, va in folds:\n",
    "        z_val = compute_embeddings(model, va, device)\n",
    "        z_train = compute_embeddings(model, tr, device)\n",
    "        z_folds.append((z_train, z_val))\n",
    "\n",
    "    fold_metrics = []\n",
    "    best_states = []\n",
    "    \n",
    "    for (train_i, val_i), (z_tr, z_va) in zip(folds, z_folds):\n",
    "        # Initialize predictor for this fold\n",
    "        predictor = LinkPredictor(\n",
    "            dim=cfg['OUT'], \n",
    "            hidden=256, \n",
    "            dropout=0.2\n",
    "        ).to(device)\n",
    "        \n",
    "        opt_head = torch.optim.Adam(\n",
    "            predictor.parameters(), \n",
    "            lr=cfg['FT_LR_HEAD'], \n",
    "            weight_decay=cfg['FT_WD_HEAD']\n",
    "        )\n",
    "        \n",
    "        best_val_auc = -1.0\n",
    "        best_state = None\n",
    "        no_improve = 0\n",
    "        \n",
    "        # Frozen backbone training\n",
    "        for ep in range(1, cfg['FT_EPOCHS_FREEZE'] + 1):\n",
    "            loss = finetune_epoch_frozen(\n",
    "                predictor, opt_head, train_i, z_tr, \n",
    "                pools_geo, pools_emb, cfg\n",
    "            )\n",
    "            \n",
    "            m_val = evaluate_edge_label(\n",
    "                predictor, z_va, val_i, \n",
    "                report_calibration=cfg['REPORT_CALIBRATION']\n",
    "            )\n",
    "            \n",
    "            # Track best model\n",
    "            if m_val['auc'] > best_val_auc:\n",
    "                best_val_auc = m_val['auc']\n",
    "                best_state = deepcopy(predictor.state_dict())\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if no_improve >= cfg['EARLY_STOP_FROZEN']:\n",
    "                break\n",
    "        \n",
    "        # Optional: Joint fine-tuning\n",
    "        if cfg['FT_EPOCHS_UNFREEZE'] > 0:\n",
    "            params = list(model.parameters()) + list(predictor.parameters())\n",
    "            opt_all = torch.optim.Adam(\n",
    "                params, \n",
    "                lr=cfg['FT_LR_UNFREEZE'], \n",
    "                weight_decay=cfg['FT_WD_UNFREEZE']\n",
    "            )\n",
    "            \n",
    "            no_improve = 0\n",
    "            \n",
    "            for ep in range(1, cfg['FT_EPOCHS_UNFREEZE'] + 1):\n",
    "                predictor.train()\n",
    "                model.train()\n",
    "                opt_all.zero_grad()\n",
    "                \n",
    "                # Recompute embeddings with trainable backbone\n",
    "                z_tr_step = compute_embeddings(model, train_i, device)\n",
    "                loss = finetune_epoch_frozen(\n",
    "                    predictor, opt_all, train_i, z_tr_step, \n",
    "                    pools_geo, pools_emb, cfg\n",
    "                )\n",
    "                \n",
    "                z_va_step = compute_embeddings(model, val_i, device)\n",
    "                m_val = evaluate_edge_label(\n",
    "                    predictor, z_va_step, val_i, \n",
    "                    report_calibration=cfg['REPORT_CALIBRATION']\n",
    "                )\n",
    "                \n",
    "                if m_val['auc'] > best_val_auc:\n",
    "                    best_val_auc = m_val['auc']\n",
    "                    best_state = deepcopy(predictor.state_dict())\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                \n",
    "                if no_improve >= cfg['EARLY_STOP_UNFREEZE']:\n",
    "                    break\n",
    "        \n",
    "        # Load best model and evaluate\n",
    "        predictor.load_state_dict(best_state)\n",
    "        z_va_final = compute_embeddings(model, val_i, device)\n",
    "        m_val_final = evaluate_edge_label(\n",
    "            predictor, z_va_final, val_i, \n",
    "            report_calibration=cfg['REPORT_CALIBRATION']\n",
    "        )\n",
    "        \n",
    "        fold_metrics.append(m_val_final)\n",
    "        best_states.append(best_state)\n",
    "\n",
    "    # =========================================================================\n",
    "    # STAGE 4: SELECT BEST FOLD AND EVALUATE ON TEST\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Choose best fold by validation AUC\n",
    "    best_idx = int(np.argmax([m['auc'] for m in fold_metrics]))\n",
    "    \n",
    "    predictor = LinkPredictor(\n",
    "        dim=cfg['OUT'], \n",
    "        hidden=256, \n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "    predictor.load_state_dict(best_states[best_idx])\n",
    "\n",
    "    # Test evaluation\n",
    "    z_test = compute_embeddings(model, test_split, device)\n",
    "    m_test = evaluate_edge_label(\n",
    "        predictor, z_test, test_split, \n",
    "        report_calibration=cfg['REPORT_CALIBRATION']\n",
    "    )\n",
    "    \n",
    "    rank_test = evaluate_ranking_candidates(\n",
    "        predictor, z_test, test_split,\n",
    "        ks=tuple(cfg['METRIC_KS']),\n",
    "        cand_geo=cfg['RANK_CAND_GEO'], \n",
    "        cand_rand=cfg['RANK_CAND_RANDOM']\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # STAGE 5: LEAKAGE ABLATION (OPTIONAL)\n",
    "    # =========================================================================\n",
    "    \n",
    "    leak = None\n",
    "    \n",
    "    if do_leakage_ablation and ('household', 'works_at', 'school') in test_split.edge_types:\n",
    "        # Remove leaking edges\n",
    "        test_filtered = remove_works_at_leak(test_split)\n",
    "        \n",
    "        # Recompute embeddings and evaluate\n",
    "        z_test_f = compute_embeddings(model, test_filtered, device)\n",
    "        m_test_f = evaluate_edge_label(\n",
    "            predictor, z_test_f, test_filtered, \n",
    "            report_calibration=cfg['REPORT_CALIBRATION']\n",
    "        )\n",
    "        \n",
    "        rank_test_f = evaluate_ranking_candidates(\n",
    "            predictor, z_test_f, test_filtered,\n",
    "            ks=tuple(cfg['METRIC_KS']),\n",
    "            cand_geo=cfg['RANK_CAND_GEO'], \n",
    "            cand_rand=cfg['RANK_CAND_RANDOM']\n",
    "        )\n",
    "        \n",
    "        leak = {\n",
    "            'original': m_test, \n",
    "            'original_rank': rank_test,\n",
    "            'filtered': m_test_f, \n",
    "            'filtered_rank': rank_test_f,\n",
    "            'delta_auc': float(m_test_f['auc'] - m_test['auc']),\n",
    "            'delta_ap': float(m_test_f['ap'] - m_test['ap']),\n",
    "        }\n",
    "\n",
    "    # =========================================================================\n",
    "    # STAGE 6: SAVE RESULTS\n",
    "    # =========================================================================\n",
    "    \n",
    "    summary = {\n",
    "        'val_folds': fold_metrics,\n",
    "        'test_uncalibrated': m_test,\n",
    "        'test_rank_uncalibrated': rank_test,\n",
    "        'leakage_ablation': leak\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(work_dir, \"robustness_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def run_robustness_suite(\n",
    "    paths: dict, \n",
    "    device: torch.device = None, \n",
    "    seed: int = 42, \n",
    "    base_cfg: dict = None, \n",
    "    output_root: str = \"./outputs\", \n",
    "    exp_name: str = \"ROBUST\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Run complete robustness evaluation suite.\n",
    "    \n",
    "    Experiments:\n",
    "    A) Standard holdout with leakage ablation\n",
    "    B) Cold-start households (inductive)\n",
    "    C) Cold-start schools (inductive)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    paths : dict\n",
    "        File paths for data CSVs.\n",
    "        \n",
    "    device : torch.device, optional\n",
    "        Computation device (auto-detected if None).\n",
    "        \n",
    "    seed : int, optional (default=42)\n",
    "        Base random seed.\n",
    "        \n",
    "    base_cfg : dict, optional\n",
    "        Configuration overrides.\n",
    "        \n",
    "    output_root : str, optional (default=\"./outputs\")\n",
    "        Output directory root.\n",
    "        \n",
    "    exp_name : str, optional (default=\"ROBUST\")\n",
    "        Experiment name prefix.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to output directory.\n",
    "    \"\"\"\n",
    "    # Auto-detect device\n",
    "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    tick(f\"[setup] Using device: {device}\")\n",
    "\n",
    "    # Load data\n",
    "    data, sizes, _, _, _ = load_and_build(paths)\n",
    "    tick(\"[Robust] Data loaded\")\n",
    "\n",
    "    # Merge configuration\n",
    "    cfg = DEFAULT_CFG.copy()\n",
    "    if base_cfg is not None:\n",
    "        cfg.update(base_cfg)\n",
    "\n",
    "    # Create output directory with timestamp\n",
    "    out_dir = os.path.join(\n",
    "        output_root, \n",
    "        f\"{exp_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPERIMENT A: Standard holdout + leakage ablation\n",
    "    # =========================================================================\n",
    "    \n",
    "    dev_A, test_A = make_fixed_test_holdout(data, test_ratio=0.10, seed=seed)\n",
    "    folds_A = make_k_dev_folds(dev_A, k=5, base_seed=1000, val_ratio=0.10)\n",
    "    \n",
    "    summ_A = train_with_splits(\n",
    "        device, data, sizes, cfg, \n",
    "        os.path.join(out_dir, \"A_standard\"), \n",
    "        seed, dev_A, folds_A, test_A, \n",
    "        do_leakage_ablation=True\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPERIMENT B: Cold-start households\n",
    "    # =========================================================================\n",
    "    \n",
    "    dev_B, folds_B, test_B = make_household_cold_start_split(\n",
    "        data, test_frac=0.20, seed=seed + 1\n",
    "    )\n",
    "    \n",
    "    summ_B = train_with_splits(\n",
    "        device, data, sizes, cfg, \n",
    "        os.path.join(out_dir, \"B_coldstart_households\"), \n",
    "        seed, dev_B, folds_B, test_B, \n",
    "        do_leakage_ablation=False\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPERIMENT C: Cold-start schools\n",
    "    # =========================================================================\n",
    "    \n",
    "    dev_C, folds_C, test_C = make_school_cold_start_split(\n",
    "        data, test_frac=0.20, seed=seed + 2\n",
    "    )\n",
    "    \n",
    "    summ_C = train_with_splits(\n",
    "        device, data, sizes, cfg, \n",
    "        os.path.join(out_dir, \"C_coldstart_schools\"), \n",
    "        seed, dev_C, folds_C, test_C, \n",
    "        do_leakage_ablation=False\n",
    "    )\n",
    "\n",
    "    # =========================================================================\n",
    "    # PRINT SUMMARY\n",
    "    # =========================================================================\n",
    "    \n",
    "    def pick(m):\n",
    "        return (m['auc'], m['ap'])\n",
    "    \n",
    "    a_auc, a_ap = pick(summ_A['test_uncalibrated'])\n",
    "    b_auc, b_ap = pick(summ_B['test_uncalibrated'])\n",
    "    c_auc, c_ap = pick(summ_C['test_uncalibrated'])\n",
    "\n",
    "    print(\"\\n=== Robustness Summary ===\")\n",
    "    print(f\"Standard test: AUC={a_auc:.4f}, AP={a_ap:.4f}\")\n",
    "    \n",
    "    if summ_A['leakage_ablation'] is not None:\n",
    "        dAUC = summ_A['leakage_ablation']['delta_auc']\n",
    "        dAP = summ_A['leakage_ablation']['delta_ap']\n",
    "        print(f\"  Leakage ablation ΔAUC={dAUC:+.4f}, ΔAP={dAP:+.4f}\")\n",
    "    else:\n",
    "        dAUC = dAP = float('nan')\n",
    "        print(\"  Leakage ablation: n/a (no works_at edges present)\")\n",
    "\n",
    "    print(f\"Cold-start households: AUC={b_auc:.4f}, AP={b_ap:.4f}\")\n",
    "    print(f\"Cold-start schools:    AUC={c_auc:.4f}, AP={c_ap:.4f}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # SAVE RESULTS\n",
    "    # =========================================================================\n",
    "    \n",
    "    essentials = {\n",
    "        'standard': {'AUC': float(a_auc), 'AP': float(a_ap)},\n",
    "        'standard_leakage_delta': {'delta_auc': float(dAUC), 'delta_ap': float(dAP)},\n",
    "        'coldstart_households': {'AUC': float(b_auc), 'AP': float(b_ap)},\n",
    "        'coldstart_schools': {'AUC': float(c_auc), 'AP': float(c_ap)}\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(out_dir, \"robustness_table_min.json\"), \"w\") as f:\n",
    "        json.dump(essentials, f, indent=2)\n",
    "\n",
    "    # Generate LaTeX table\n",
    "    tex_path = os.path.join(out_dir, \"robustness_table_min.tex\")\n",
    "    with open(tex_path, \"w\") as f:\n",
    "        f.write(\"\\\\begin{tabular}{lrrrr}\\n\\\\toprule\\n\")\n",
    "        f.write(\"Setting & AUC & AP & $\\\\Delta$AUC & $\\\\Delta$AP \\\\\\\\\\n\\\\midrule\\n\")\n",
    "        f.write(f\"Standard & {a_auc:.3f} & {a_ap:.3f} & -- & -- \\\\\\\\\\n\")\n",
    "        \n",
    "        if np.isfinite(dAUC) and np.isfinite(dAP):\n",
    "            f.write(f\"Standard (w/o works\\\\_at matches) & {a_auc + dAUC:.3f} & {a_ap + dAP:.3f} & {dAUC:+.3f} & {dAP:+.3f} \\\\\\\\\\n\")\n",
    "        else:\n",
    "            f.write(f\"Standard (w/o works\\\\_at matches) & n/a & n/a & n/a & n/a \\\\\\\\\\n\")\n",
    "        \n",
    "        f.write(f\"Cold-start households & {b_auc:.3f} & {b_ap:.3f} & {b_auc - a_auc:+.3f} & {b_ap - a_ap:+.3f} \\\\\\\\\\n\")\n",
    "        f.write(f\"Cold-start schools & {c_auc:.3f} & {c_ap:.3f} & {c_auc - a_auc:+.3f} & {c_ap - a_ap:+.3f} \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\")\n",
    "\n",
    "    print(f\"\\n[Robustness artifacts saved under] {out_dir}\")\n",
    "    print(f\"LaTeX table: {tex_path}\")\n",
    "    \n",
    "    return out_dir\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 15: COMMAND-LINE INTERFACE\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for command-line execution.\n",
    "    \n",
    "    Usage:\n",
    "        python geo_social_robust.py --mode robust --seed 42 --device cuda\n",
    "        \n",
    "    Arguments:\n",
    "        --mode: Execution mode ('robust' for robustness suite)\n",
    "        --outputs: Output directory root\n",
    "        --exp: Experiment name prefix\n",
    "        --households: Path to household CSV\n",
    "        --students: Path to student CSV\n",
    "        --staff: Path to staff CSV\n",
    "        --seed: Random seed\n",
    "        --device: Computation device ('cuda' or 'cpu')\n",
    "    \"\"\"\n",
    "    # Define command-line arguments\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Geo-Social Graph Learning + Robustness Testing\",\n",
    "        add_help=True\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--mode\", type=str, default=\"robust\", choices=[\"robust\"],\n",
    "        help=\"Execution mode: 'robust' runs leakage ablation and cold-start tests\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--outputs\", type=str, default=\"./outputs\",\n",
    "        help=\"Root directory for output files\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--exp\", type=str, default=\"ROBUST\",\n",
    "        help=\"Experiment name prefix for output folder\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--households\", type=str, \n",
    "        default=\"hui_v0-1-0_Lumberton_NC_2010_rs9876.csv\",\n",
    "        help=\"Path to household CSV file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--students\", type=str,\n",
    "        default=\"prec_v0-2-0_Lumberton_NC_2010_rs9876_students.csv\",\n",
    "        help=\"Path to student CSV file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--staff\", type=str,\n",
    "        default=\"prec_v0-2-0_Lumberton_NC_2010_rs9876_schoolstaff.csv\",\n",
    "        help=\"Path to staff CSV file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=42,\n",
    "        help=\"Random seed for reproducibility\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str,\n",
    "        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        help=\"Computation device: 'cuda' or 'cpu'\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-f\", \"--f\", default=None,\n",
    "        help=\"(Ignored, for Jupyter compatibility)\"\n",
    "    )\n",
    "    \n",
    "    # Parse arguments\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Build paths dictionary\n",
    "    paths = dict(\n",
    "        households=args.households,\n",
    "        students=args.students,\n",
    "        staff=args.staff\n",
    "    )\n",
    "    \n",
    "    # Warn about missing files\n",
    "    for p in paths.values():\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[WARN] Not found: {p} (CWD={os.getcwd()}). Use absolute path if needed.\")\n",
    "\n",
    "    # Configure device\n",
    "    device = torch.device(\n",
    "        args.device if args.device in [\"cpu\", \"cuda\"] \n",
    "        else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    )\n",
    "\n",
    "    # Run robustness suite\n",
    "    if args.mode == \"robust\":\n",
    "        run_robustness_suite(\n",
    "            paths=paths, \n",
    "            device=device, \n",
    "            seed=args.seed, \n",
    "            base_cfg=None,\n",
    "            output_root=args.outputs, \n",
    "            exp_name=args.exp\n",
    "        )\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SCRIPT ENTRY POINT\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee5014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using experiment dir: ./outputs\\FULL_20250819_130311\n",
      "\n",
      "===== LaTeX tables written to =====\n",
      "./outputs\\paper_tables.tex\n",
      "\n",
      "===== Preview (first 1500 chars) =====\n",
      "\\begin{table}[H]\\centering\n",
      "\\caption{Data overview (Lumberton, 2010).}\n",
      "\\label{tab:data-overview}\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "Quantity & Value & Notes \\\\\n",
      "\\midrule\n",
      "Households ($|\\mathcal{V}_h|$) & 4682 & non-missing attributes and coords \\\\\n",
      "Schools ($|\\mathcal{V}_s|$) & 8 & deduplicated by \\texttt{NCESSCH} \\\\\n",
      "Attendance edges ($|\\mathcal{E}_{\\texttt{attends}}|$) & 4183 & unique $(h,s)$ pairs \\\\\n",
      "Employment edges ($|\\mathcal{E}_{\\texttt{works\\_at}}|$) & 0 & optional \\\\\n",
      "Bipartite density $\\rho$ & 0.111678 & $|\\mathcal{E}_{\\texttt{attends}}|/(|\\mathcal{V}_h||\\mathcal{V}_s|)$ \\\\\n",
      "Median deg$(h)$ / deg$(s)$ & 1.000 / 389.500 & on \\texttt{attends} \\\\\n",
      "Average deg$(h)$ / deg$(s)$ & 1.218 / 522.875 & on \\texttt{attends} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[H]\\centering\n",
      "\\caption{Test metrics per seed.}\n",
      "\\label{tab:test-per-seed}\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "Seed & AUC & AP & Brier & ECE & F1 (thr) \\\\\n",
      "\\midrule\n",
      "seed_42 & 0.994968 & 0.994070 & 0.030565 & 0.077424 & 0.975962 (0.465) \\\\\n",
      "seed_43 & 0.998008 & 0.997885 & 0.029400 & 0.087105 & 0.982101 (0.325) \\\\\n",
      "seed_44 & 0.997340 & 0.997090 & 0.028493 & 0.080567 & 0.979710 (0.395) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}[H]\\centering\n",
      "\\caption{Test metrics aggregated across seeds (mean $\\pm$ std).}\n",
      "\\label{tab:test-agg}\n",
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "Metric & Mean $\\pm$ Std &  \\\\\n",
      "\\midrule\n",
      "AUC & 0.997$\\pm$0.002 & \\\\\n",
      "AP & 0.996$\\pm$0.002 & \\\\\n",
      "Brier & 0.029$\\pm$0.001 & \\\\\n",
      "ECE & 0.082$\\pm$0.005 & \\\\\n",
      "F1  & 0.979$\\pm$\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PUBLICATION-READY LATEX TABLES GENERATOR\n",
    "================================================================================\n",
    "\n",
    "PAPER REFERENCE\n",
    "---------------\n",
    "This module supports the paper:\n",
    "\n",
    "    \"Calibrated geo-social link prediction for household–school connectivity\n",
    "     in community resilience\"\n",
    "    Gupta, H.S., Biswas, S., & Nicholson, C.D.\n",
    "    International Journal of Disaster Risk Reduction, Volume 131 (2025)\n",
    "    DOI: https://doi.org/10.1016/j.ijdrr.2025.105872\n",
    "\n",
    "OVERVIEW\n",
    "--------\n",
    "Generates camera-ready LaTeX tables for academic publication from the \n",
    "experimental results produced by the GNN training pipeline. All tables are \n",
    "formatted according to journal requirements with:\n",
    "    - Proper captioning and labeling for cross-references\n",
    "    - Consistent decimal precision and formatting\n",
    "    - Mean ± standard deviation reporting for aggregated results\n",
    "\n",
    "GENERATED TABLES\n",
    "----------------\n",
    "    Table 1: DATA OVERVIEW\n",
    "             Graph statistics including node/edge counts, density, and degree\n",
    "             distributions for the Lumberton synthetic population.\n",
    "             \n",
    "    Table 2: PER-SEED PERFORMANCE\n",
    "             Individual experiment results showing AUC, AP, Brier, ECE, and F1\n",
    "             for each random seed to demonstrate reproducibility.\n",
    "             \n",
    "    Table 3: AGGREGATED PERFORMANCE\n",
    "             Mean ± std across all seeds for primary evaluation metrics.\n",
    "             \n",
    "    Table 4: RANKING METRICS\n",
    "             Hit@k and NDCG@k for k ∈ {1, 3, 5, 10} to evaluate the model's\n",
    "             ability to rank true positive schools highly.\n",
    "             \n",
    "    Table 5: FAIRNESS DIAGNOSTICS\n",
    "             Subgroup performance analysis across demographic groups to\n",
    "             identify potential disparities.\n",
    "             \n",
    "    Table 6: ROBUSTNESS RESULTS\n",
    "             Leakage ablation (removing works_at edge shortcuts) and\n",
    "             cold-start evaluation (unseen households/schools).\n",
    "\n",
    "Authors: Himadri Sen Gupta, Saptadeep Biswas, Charles D. Nicholson\n",
    "Version: 1.0.0\n",
    "License: MIT\n",
    "\n",
    "Usage:\n",
    "    # From Python/Jupyter:\n",
    "    >>> main()  # Generates paper_tables.tex in outputs directory\n",
    "    \n",
    "    # Output can be directly included in LaTeX document:\n",
    "    # \\\\input{outputs/paper_tables.tex}\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: IMPORTS AND CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Standard library imports\n",
    "import os      # Operating system interface for file/path operations\n",
    "import glob    # Unix-style pathname pattern expansion\n",
    "import json    # JSON encoding/decoding for loading results\n",
    "\n",
    "# Numerical computing and data manipulation\n",
    "import numpy as np    # Numerical arrays and mathematical operations\n",
    "import pandas as pd   # DataFrames for tabular data processing\n",
    "\n",
    "# Evaluation metrics from scikit-learn\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,           # Area Under ROC Curve\n",
    "    average_precision_score, # Average Precision\n",
    "    f1_score,                # F1 Score (harmonic mean of precision and recall)\n",
    "    brier_score_loss         # Brier score for probability calibration\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: CONFIGURATION CONSTANTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Data file paths - UPDATE THESE TO MATCH YOUR DATA LOCATION\n",
    "HOUSEHOLDS = \"hui_v0-1-0_Lumberton_NC_2010_rs9876.csv\"      # Household demographics\n",
    "STUDENTS = \"prec_v0-2-0_Lumberton_NC_2010_rs9876_students.csv\"  # Student enrollment\n",
    "STAFF = \"prec_v0-2-0_Lumberton_NC_2010_rs9876_schoolstaff.csv\"  # Staff employment\n",
    "\n",
    "# Output configuration\n",
    "OUTPUTS_DIR = \"./outputs\"   # Directory containing FULL_* and ROBUST_* results\n",
    "\n",
    "# Evaluation parameters\n",
    "ECE_BINS = 15  # Number of bins for Expected Calibration Error calculation\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def find_latest_dir(root: str, prefix: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the most recently modified directory matching a prefix.\n",
    "    \n",
    "    Searches for directories matching the pattern {root}/{prefix}_* and\n",
    "    returns the most recently modified one. This is useful for finding\n",
    "    the latest experiment results.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "        Root directory to search in.\n",
    "        \n",
    "    prefix : str\n",
    "        Directory name prefix to match (e.g., \"FULL\", \"ROBUST\").\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        Path to the most recent matching directory, or None if not found.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> find_latest_dir(\"./outputs\", \"FULL\")\n",
    "    './outputs/FULL_20240115_143022'\n",
    "    \"\"\"\n",
    "    # Find all directories matching the pattern\n",
    "    cand = [p for p in glob.glob(os.path.join(root, f\"{prefix}_*\")) if os.path.isdir(p)]\n",
    "    \n",
    "    # Return None if no matches\n",
    "    if not cand:\n",
    "        return None\n",
    "    \n",
    "    # Sort by modification time (most recent first)\n",
    "    cand.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    \n",
    "    return cand[0]\n",
    "\n",
    "\n",
    "def expected_calibration_error(\n",
    "    probs: np.ndarray, \n",
    "    labels: np.ndarray, \n",
    "    n_bins: int = 15\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute Expected Calibration Error (ECE) for probability predictions.\n",
    "    \n",
    "    ECE measures how well predicted probabilities align with observed\n",
    "    frequencies. A perfectly calibrated model has ECE = 0.\n",
    "    \n",
    "    Formula:\n",
    "        ECE = Σ (|B_i| / N) * |acc(B_i) - conf(B_i)|\n",
    "        \n",
    "    where B_i are bins, acc is accuracy, and conf is average confidence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    probs : np.ndarray\n",
    "        Predicted probabilities in [0, 1].\n",
    "        \n",
    "    labels : np.ndarray\n",
    "        Binary ground truth labels (0 or 1).\n",
    "        \n",
    "    n_bins : int, optional (default=15)\n",
    "        Number of equal-width bins for calibration.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Expected Calibration Error in [0, 1].\n",
    "        \n",
    "    Reference\n",
    "    ---------\n",
    "    Naeini et al. (2015) \"Obtaining Well Calibrated Probabilities Using\n",
    "    Bayesian Binning into Quantiles\"\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "    >>> probs = np.array([0.9, 0.8, 0.3, 0.2])\n",
    "    >>> labels = np.array([1, 1, 0, 0])\n",
    "    >>> expected_calibration_error(probs, labels)\n",
    "    0.05\n",
    "    \"\"\"\n",
    "    # Ensure numpy arrays with correct dtype\n",
    "    probs = np.asarray(probs, dtype=float)\n",
    "    labels = np.asarray(labels, dtype=float)\n",
    "    \n",
    "    # Create equal-width bins from 0 to 1\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    \n",
    "    # Assign each prediction to a bin\n",
    "    inds = np.digitize(probs, bins) - 1\n",
    "    \n",
    "    # Calculate ECE\n",
    "    ece = 0.0\n",
    "    N = len(probs)\n",
    "    \n",
    "    for b in range(n_bins):\n",
    "        # Get predictions in this bin\n",
    "        mask = inds == b\n",
    "        \n",
    "        if np.any(mask):\n",
    "            # Average confidence in bin\n",
    "            conf = probs[mask].mean()\n",
    "            \n",
    "            # Accuracy in bin (fraction of positive labels)\n",
    "            acc = labels[mask].mean()\n",
    "            \n",
    "            # Add weighted absolute difference\n",
    "            ece += abs(acc - conf) * (mask.sum() / N)\n",
    "    \n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def format_pm(mean: float, std: float, prec: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Format mean and standard deviation as LaTeX \"mean ± std\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float\n",
    "        Mean value.\n",
    "        \n",
    "    std : float\n",
    "        Standard deviation.\n",
    "        \n",
    "    prec : int, optional (default=3)\n",
    "        Decimal precision.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        LaTeX-formatted string like \"0.850$\\\\pm$0.012\".\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> format_pm(0.8523, 0.0124, prec=3)\n",
    "    '0.852$\\\\pm$0.012'\n",
    "    \"\"\"\n",
    "    return f\"{mean:.{prec}f}$\\\\pm${std:.{prec}f}\"\n",
    "\n",
    "\n",
    "def safe(num, prec: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Safely format a number for LaTeX output.\n",
    "    \n",
    "    Handles numeric types and converts to string representation.\n",
    "    Non-numeric values are returned as-is.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num : any\n",
    "        Value to format.\n",
    "        \n",
    "    prec : int, optional (default=3)\n",
    "        Decimal precision for numeric values.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Formatted string representation.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> safe(0.12345, prec=3)\n",
    "    '0.123'\n",
    "    >>> safe(\"N/A\")\n",
    "    'N/A'\n",
    "    \"\"\"\n",
    "    # Check if numeric type\n",
    "    if isinstance(num, (int, float, np.floating)):\n",
    "        return f\"{num:.{prec}f}\"\n",
    "    return str(num)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: TABLE 1 - DATA OVERVIEW\n",
    "# ==============================================================================\n",
    "\n",
    "def data_overview(\n",
    "    households_csv: str, \n",
    "    students_csv: str, \n",
    "    staff_csv: str\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compute comprehensive data statistics for Table 1.\n",
    "    \n",
    "    Analyzes the input CSV files to extract graph statistics including:\n",
    "    - Number of nodes (households, schools)\n",
    "    - Number of edges (attends, works_at)\n",
    "    - Graph density\n",
    "    - Degree statistics\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    households_csv : str\n",
    "        Path to household CSV file.\n",
    "        \n",
    "    students_csv : str\n",
    "        Path to student CSV file.\n",
    "        \n",
    "    staff_csv : str\n",
    "        Path to staff CSV file.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing all statistics for the data overview table.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If required columns are missing from CSV files.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 1: Load CSV files\n",
    "    # -------------------------------------------------------------------------\n",
    "    df_h = pd.read_csv(households_csv)\n",
    "    df_s = pd.read_csv(students_csv)\n",
    "    \n",
    "    # Staff file is optional\n",
    "    df_t = pd.read_csv(staff_csv) if os.path.exists(staff_csv) else pd.DataFrame()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 2: Clean household data\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Required columns for analysis\n",
    "    need_cols = ['ownershp', 'race', 'hispan', 'randincome']\n",
    "    df_h = df_h.dropna(subset=need_cols).copy()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 3: Merge coordinates from students/staff\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Extract household coordinates from students\n",
    "    stc = (\n",
    "        df_s[['huid', 'hcb_lat', 'hcb_lon']].drop_duplicates('huid')\n",
    "        if {'huid', 'hcb_lat', 'hcb_lon'}.issubset(df_s.columns) \n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    \n",
    "    # Extract household coordinates from staff\n",
    "    ttc = (\n",
    "        df_t[['huid', 'hcb_lat', 'hcb_lon']].drop_duplicates('huid')\n",
    "        if not df_t.empty and {'huid', 'hcb_lat', 'hcb_lon'}.issubset(df_t.columns) \n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    \n",
    "    # Combine coordinate sources\n",
    "    allc = pd.concat([stc, ttc], ignore_index=True).drop_duplicates('huid').set_index('huid')\n",
    "    \n",
    "    # Join coordinates to households\n",
    "    if not allc.empty:\n",
    "        df_h = df_h.join(allc, on='huid')\n",
    "        df_h = df_h.dropna(subset=['hcb_lat', 'hcb_lon'])\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 4: Extract school information\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Check for required columns\n",
    "    if not {'NCESSCH', 'SCHNAM09', 'ncs_lat', 'ncs_lon'}.issubset(df_s.columns):\n",
    "        raise RuntimeError(\"Students CSV must have NCESSCH, SCHNAM09, ncs_lat, ncs_lon\")\n",
    "    \n",
    "    df_sch = df_s[['NCESSCH', 'SCHNAM09', 'ncs_lat', 'ncs_lon']].drop_duplicates('NCESSCH')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 5: Build ID sets for edge filtering\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    hh_ids = sorted(df_h['huid'].unique().tolist())\n",
    "    sc_ids = sorted(df_sch['NCESSCH'].unique().tolist())\n",
    "    hh_set = set(hh_ids)\n",
    "    sc_set = set(sc_ids)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 6: Count attendance edges\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    if not {'huid', 'NCESSCH'}.issubset(df_s.columns):\n",
    "        raise RuntimeError(\"Students CSV must have huid and NCESSCH\")\n",
    "    \n",
    "    # Filter to valid households and schools, deduplicate\n",
    "    df_att = (\n",
    "        df_s.loc[df_s['huid'].isin(hh_set) & df_s['NCESSCH'].isin(sc_set), ['huid', 'NCESSCH']]\n",
    "        .dropna()\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "    n_att = int(len(df_att))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 7: Count employment edges (optional)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    n_work = 0\n",
    "    \n",
    "    if not df_t.empty:\n",
    "        # Create school name to ID mapping\n",
    "        schmap = df_sch.set_index('SCHNAM09')['NCESSCH']\n",
    "        \n",
    "        # Try to find school column\n",
    "        col_name_guess = None\n",
    "        for c in ['NCESSCH', 'SIName', 'school', 'School', 'SCHNAM09']:\n",
    "            if c in df_t.columns:\n",
    "                col_name_guess = c\n",
    "                break\n",
    "        \n",
    "        if col_name_guess is not None:\n",
    "            df_t = df_t.copy()\n",
    "            \n",
    "            # Map school names to IDs if needed\n",
    "            if col_name_guess != 'NCESSCH':\n",
    "                df_t['NCESSCH'] = df_t[col_name_guess].map(schmap)\n",
    "            \n",
    "            # Count valid employment edges\n",
    "            df_work = df_t[['huid', 'NCESSCH']].dropna().drop_duplicates()\n",
    "            df_work = df_work.loc[df_work['huid'].isin(hh_set) & df_work['NCESSCH'].isin(sc_set)]\n",
    "            n_work = int(len(df_work))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 8: Compute graph statistics\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Node counts\n",
    "    Vh, Vs = len(hh_ids), len(sc_ids)\n",
    "    \n",
    "    # Bipartite density: edges / (households × schools)\n",
    "    density = n_att / (Vh * Vs) if (Vh > 0 and Vs > 0) else float('nan')\n",
    "    \n",
    "    # Degree distributions\n",
    "    deg_h = df_att.groupby('huid').size() if n_att > 0 else pd.Series(dtype=int)\n",
    "    deg_s = df_att.groupby('NCESSCH').size() if n_att > 0 else pd.Series(dtype=int)\n",
    "    \n",
    "    # Median degrees\n",
    "    med_deg_h = float(deg_h.median()) if not deg_h.empty else float('nan')\n",
    "    med_deg_s = float(deg_s.median()) if not deg_s.empty else float('nan')\n",
    "    \n",
    "    # Average degrees\n",
    "    avg_deg_h = float(deg_h.mean()) if not deg_h.empty else float('nan')\n",
    "    avg_deg_s = float(deg_s.mean()) if not deg_s.empty else float('nan')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # STEP 9: Return statistics dictionary\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    overview = dict(\n",
    "        households=Vh,\n",
    "        schools=Vs,\n",
    "        attends=n_att,\n",
    "        works_at=n_work,\n",
    "        density=density,\n",
    "        median_deg_h=med_deg_h,\n",
    "        median_deg_s=med_deg_s,\n",
    "        avg_deg_h=avg_deg_h,\n",
    "        avg_deg_s=avg_deg_s\n",
    "    )\n",
    "    \n",
    "    return overview\n",
    "\n",
    "\n",
    "def render_table1_tex(stats: dict) -> str:\n",
    "    \"\"\"\n",
    "    Render Table 1 (Data Overview) as LaTeX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stats : dict\n",
    "        Statistics dictionary from data_overview().\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Complete LaTeX table environment string.\n",
    "    \"\"\"\n",
    "    return rf\"\"\"\\begin{{table}}[H]\\centering\n",
    "\\caption{{Data overview (Lumberton, 2010).}}\n",
    "\\label{{tab:data-overview}}\n",
    "\\begin{{tabular}}{{lrr}}\n",
    "\\toprule\n",
    "Quantity & Value & Notes \\\\\n",
    "\\midrule\n",
    "Households ($|\\mathcal{{V}}_h|$) & {stats['households']} & non-missing attributes and coords \\\\\n",
    "Schools ($|\\mathcal{{V}}_s|$) & {stats['schools']} & deduplicated by \\texttt{{NCESSCH}} \\\\\n",
    "Attendance edges ($|\\mathcal{{E}}_{{\\texttt{{attends}}}}|$) & {stats['attends']} & unique $(h,s)$ pairs \\\\\n",
    "Employment edges ($|\\mathcal{{E}}_{{\\texttt{{works\\_at}}}}|$) & {stats['works_at']} & optional \\\\\n",
    "Bipartite density $\\rho$ & {stats['density']:.6f} & $|\\mathcal{{E}}_{{\\texttt{{attends}}}}|/(|\\mathcal{{V}}_h||\\mathcal{{V}}_s|)$ \\\\\n",
    "Median deg$(h)$ / deg$(s)$ & {safe(stats['median_deg_h'])} / {safe(stats['median_deg_s'])} & on \\texttt{{attends}} \\\\\n",
    "Average deg$(h)$ / deg$(s)$ & {safe(stats['avg_deg_h'])} / {safe(stats['avg_deg_s'])} & on \\texttt{{attends}} \\\\\n",
    "\\bottomrule\n",
    "\\end{{tabular}}\n",
    "\\end{{table}}\"\"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 5: TABLES 2 & 3 - PER-SEED AND AGGREGATED PERFORMANCE\n",
    "# ==============================================================================\n",
    "\n",
    "def compute_seed_metrics(seed_dir: str, ece_bins: int = 15) -> dict:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for a single seed from test_edge_scores.csv.\n",
    "    \n",
    "    Computes comprehensive metrics including:\n",
    "    - AUC-ROC: Discrimination ability\n",
    "    - Average Precision: Performance on positive class\n",
    "    - Brier Score: Calibration + refinement\n",
    "    - ECE: Expected Calibration Error\n",
    "    - Best F1: Maximum F1 with optimal threshold\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_dir : str\n",
    "        Directory containing test_edge_scores.csv for this seed.\n",
    "        \n",
    "    ece_bins : int, optional (default=15)\n",
    "        Number of bins for ECE calculation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with all computed metrics.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    FileNotFoundError\n",
    "        If test_edge_scores.csv is not found.\n",
    "    \"\"\"\n",
    "    # Load predictions\n",
    "    csv_path = os.path.join(seed_dir, \"test_edge_scores.csv\")\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(csv_path)\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Extract labels and probabilities\n",
    "    y = df['label'].to_numpy().astype(int)\n",
    "    p = df['prob'].to_numpy().astype(float)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute discrimination metrics\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    auc = roc_auc_score(y, p)\n",
    "    ap = average_precision_score(y, p)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute calibration metrics\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    brier = brier_score_loss(y, p)\n",
    "    ece = expected_calibration_error(p, y, n_bins=ece_bins)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Find optimal F1 threshold\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    thresholds = np.linspace(0, 1, 201)  # Test 201 thresholds\n",
    "    f1s = []\n",
    "    \n",
    "    for t in thresholds:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        f1s.append(f1_score(y, yhat))\n",
    "    \n",
    "    # Find best F1 and corresponding threshold\n",
    "    idx = int(np.argmax(f1s))\n",
    "    best_f1 = float(f1s[idx])\n",
    "    best_thr = float(thresholds[idx])\n",
    "    \n",
    "    return dict(\n",
    "        AUC=auc,\n",
    "        AP=ap,\n",
    "        Brier=brier,\n",
    "        ECE=ece,\n",
    "        F1=best_f1,\n",
    "        thr=best_thr\n",
    "    )\n",
    "\n",
    "\n",
    "def gather_all_seed_metrics(exp_dir: str, ece_bins: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gather metrics from all seed directories in an experiment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    exp_dir : str\n",
    "        Experiment directory containing seed_* subdirectories.\n",
    "        \n",
    "    ece_bins : int, optional (default=15)\n",
    "        Number of bins for ECE calculation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with one row per seed, columns for each metric.\n",
    "    \"\"\"\n",
    "    # Find all seed directories\n",
    "    seeds = sorted([\n",
    "        d for d in glob.glob(os.path.join(exp_dir, \"seed_*\")) \n",
    "        if os.path.isdir(d)\n",
    "    ])\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for sd in seeds:\n",
    "        name = os.path.basename(sd)\n",
    "        try:\n",
    "            m = compute_seed_metrics(sd, ece_bins)\n",
    "            rows.append(dict(seed=name, **m))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] skipping {name}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def render_table2_tex(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Render Table 2 (Per-Seed Performance) as LaTeX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame from gather_all_seed_metrics().\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Complete LaTeX table environment string.\n",
    "    \"\"\"\n",
    "    # Build table rows\n",
    "    lines = []\n",
    "    for _, r in df.sort_values('seed').iterrows():\n",
    "        lines.append(\n",
    "            f\"{r['seed']} & {r['AUC']:.6f} & {r['AP']:.6f} & \"\n",
    "            f\"{r['Brier']:.6f} & {r['ECE']:.6f} & {r['F1']:.6f} ({r['thr']:.3f}) \\\\\\\\\"\n",
    "        )\n",
    "    \n",
    "    body = \"\\n\".join(lines)\n",
    "    \n",
    "    return rf\"\"\"\\begin{{table}}[H]\\centering\n",
    "\\caption{{Test metrics per seed.}}\n",
    "\\label{{tab:test-per-seed}}\n",
    "\\begin{{tabular}}{{lrrrrr}}\n",
    "\\toprule\n",
    "Seed & AUC & AP & Brier & ECE & F1 (thr) \\\\\n",
    "\\midrule\n",
    "{body}\n",
    "\\bottomrule\n",
    "\\end{{tabular}}\n",
    "\\end{{table}}\"\"\"\n",
    "\n",
    "\n",
    "def render_table3_tex(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Render Table 3 (Aggregated Performance) as LaTeX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame from gather_all_seed_metrics().\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Complete LaTeX table environment string.\n",
    "    \"\"\"\n",
    "    # Compute aggregates\n",
    "    agg = df[['AUC', 'AP', 'Brier', 'ECE', 'F1']].agg(['mean', 'std'])\n",
    "    \n",
    "    return rf\"\"\"\\begin{{table}}[H]\\centering\n",
    "\\caption{{Test metrics aggregated across seeds (mean $\\pm$ std).}}\n",
    "\\label{{tab:test-agg}}\n",
    "\\begin{{tabular}}{{lrr}}\n",
    "\\toprule\n",
    "Metric & Mean $\\pm$ Std &  \\\\\n",
    "\\midrule\n",
    "AUC & {format_pm(agg.loc['mean','AUC'], agg.loc['std','AUC'])} & \\\\\n",
    "AP & {format_pm(agg.loc['mean','AP'], agg.loc['std','AP'])} & \\\\\n",
    "Brier & {format_pm(agg.loc['mean','Brier'], agg.loc['std','Brier'])} & \\\\\n",
    "ECE & {format_pm(agg.loc['mean','ECE'], agg.loc['std','ECE'])} & \\\\\n",
    "F1  & {format_pm(agg.loc['mean','F1'], agg.loc['std','F1'])} & \\\\\n",
    "\\bottomrule\n",
    "\\end{{tabular}}\n",
    "\\end{{table}}\"\"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 6: TABLE 4 - RANKING METRICS\n",
    "# ==============================================================================\n",
    "\n",
    "def gather_ranking_metrics(\n",
    "    exp_dir: str, \n",
    "    ks: tuple = (1, 3, 5, 10)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gather ranking metrics (Hit@k, NDCG@k) from all seeds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    exp_dir : str\n",
    "        Experiment directory containing seed_* subdirectories.\n",
    "        \n",
    "    ks : tuple, optional (default=(1,3,5,10))\n",
    "        k values for ranking metrics.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with ranking metrics per seed.\n",
    "    \"\"\"\n",
    "    # Find all seed directories\n",
    "    seeds = sorted([\n",
    "        d for d in glob.glob(os.path.join(exp_dir, \"seed_*\")) \n",
    "        if os.path.isdir(d)\n",
    "    ])\n",
    "    \n",
    "    recs = []\n",
    "    \n",
    "    for sd in seeds:\n",
    "        summ = os.path.join(sd, \"summary.json\")\n",
    "        \n",
    "        if not os.path.exists(summ):\n",
    "            print(f\"[WARN] no summary.json in {sd}\")\n",
    "            continue\n",
    "        \n",
    "        with open(summ, \"r\") as f:\n",
    "            J = json.load(f)\n",
    "        \n",
    "        rk = J.get('test_rank_uncalibrated', {})\n",
    "        row = {'seed': os.path.basename(sd)}\n",
    "        \n",
    "        # Extract metrics for each k\n",
    "        for k in ks:\n",
    "            row[f'hit@{k}'] = rk.get(f'hit@{k}', np.nan)\n",
    "            row[f'ndcg@{k}'] = rk.get(f'ndcg@{k}', np.nan)\n",
    "        \n",
    "        recs.append(row)\n",
    "    \n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "\n",
    "def render_table4_tex(df_rank: pd.DataFrame, ks: tuple = (1, 3, 5, 10)) -> str:\n",
    "    \"\"\"\n",
    "    Render Table 4 (Ranking Metrics) as LaTeX.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_rank : pd.DataFrame\n",
    "        DataFrame from gather_ranking_metrics().\n",
    "        \n",
    "    ks : tuple, optional (default=(1,3,5,10))\n",
    "        k values for ranking metrics.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Complete LaTeX table environment string.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    # Hit@k row\n",
    "    hit_ms = []\n",
    "    for k in ks:\n",
    "        m = df_rank[f'hit@{k}'].mean()\n",
    "        s = df_rank[f'hit@{k}'].std()\n",
    "        hit_ms.append(f\"{format_pm(m, s, prec=3)}\")\n",
    "    lines.append(\"Hit & \" + \" & \".join(hit_ms) + r\" \\\\\")\n",
    "    \n",
    "    # NDCG@k row\n",
    "    ndcg_ms = []\n",
    "    for k in ks:\n",
    "        m = df_rank[f'ndcg@{k}'].mean()\n",
    "        s = df_rank[f'ndcg@{k}'].std()\n",
    "        ndcg_ms.append(f\"{format_pm(m, s, prec=3)}\")\n",
    "    lines.append(\"NDCG & \" + \" & \".join(ndcg_ms) + r\" \\\\\")\n",
    "    \n",
    "    body = \"\\n\".join(lines)\n",
    "    headers = \" & \".join([f\"@{k}\" for k in ks])\n",
    "    \n",
    "    return rf\"\"\"\\begin{{table}}[H]\\centering\n",
    "\\caption{{Candidate ranking metrics on the test set (mean $\\pm$ std across seeds).}}\n",
    "\\label{{tab:ranking}}\n",
    "\\begin{{tabular}}{{l{('r'*len(ks))}}}\n",
    "\\toprule\n",
    "Metric & {headers} \\\\\n",
    "\\midrule\n",
    "{body}\n",
    "\\bottomrule\n",
    "\\end{{tabular}}\n",
    "\\end{{table}}\"\"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 7: TABLES 5 & 6 - FAIRNESS AND ROBUSTNESS (VIA \\input)\n",
    "# ==============================================================================\n",
    "\n",
    "def find_fairness_tex(exp_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Find fairness_by_group.tex file in experiment directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    exp_dir : str\n",
    "        Experiment directory to search.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        Path to fairness LaTeX file, or None if not found.\n",
    "    \"\"\"\n",
    "    cand = glob.glob(os.path.join(exp_dir, \"fairness_by_group.tex\"))\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "\n",
    "def find_robustness_tex(root: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Find robustness_table_min.tex in the latest ROBUST directory.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "        Root outputs directory.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (robustness_dir, tex_path) or (dir, None) if not found.\n",
    "    \"\"\"\n",
    "    rob_dir = find_latest_dir(root, \"ROBUST\")\n",
    "    \n",
    "    if not rob_dir:\n",
    "        return None, None\n",
    "    \n",
    "    cand = glob.glob(os.path.join(rob_dir, \"robustness_table_min.tex\"))\n",
    "    \n",
    "    return (rob_dir, cand[0]) if cand else (rob_dir, None)\n",
    "\n",
    "\n",
    "def wrap_input_table(caption: str, label: str, relpath: str) -> str:\n",
    "    \"\"\"\n",
    "    Wrap a LaTeX \\input command in a table environment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    caption : str\n",
    "        Table caption text.\n",
    "        \n",
    "    label : str\n",
    "        LaTeX label for cross-referencing.\n",
    "        \n",
    "    relpath : str\n",
    "        Relative path to the .tex file to include.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Complete LaTeX table environment with \\input.\n",
    "    \"\"\"\n",
    "    return rf\"\"\"\\begin{{table}}[H]\\centering\n",
    "\\caption{{{caption}}}\n",
    "\\label{{{label}}}\n",
    "\\input{{{relpath}}}\n",
    "\\end{{table}}\"\"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 8: MAIN ORCHESTRATION FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to generate all publication tables.\n",
    "    \n",
    "    Orchestrates the generation of:\n",
    "    - Table 1: Data Overview\n",
    "    - Table 2: Per-Seed Performance\n",
    "    - Table 3: Aggregated Performance\n",
    "    - Table 4: Ranking Metrics\n",
    "    - Table 5: Fairness (via \\input)\n",
    "    - Table 6: Robustness (via \\input)\n",
    "    \n",
    "    Outputs:\n",
    "    - paper_tables.tex: Combined LaTeX file with all tables\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Find experiment directory\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    exp_dir = find_latest_dir(OUTPUTS_DIR, \"FULL\")\n",
    "    \n",
    "    if not exp_dir:\n",
    "        raise RuntimeError(f\"No FULL_* experiment found under {OUTPUTS_DIR}.\")\n",
    "    \n",
    "    print(f\"[INFO] Using experiment dir: {exp_dir}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Table 1: Data Overview\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    t1_stats = data_overview(HOUSEHOLDS, STUDENTS, STAFF)\n",
    "    t1_tex = render_table1_tex(t1_stats)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Tables 2 & 3: Per-Seed and Aggregated Performance\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    per_seed = gather_all_seed_metrics(exp_dir, ece_bins=ECE_BINS)\n",
    "    \n",
    "    if per_seed.empty:\n",
    "        raise RuntimeError(\n",
    "            \"No per-seed metrics found; ensure test_edge_scores.csv \"\n",
    "            \"exists in seed_* subfolders.\"\n",
    "        )\n",
    "    \n",
    "    t2_tex = render_table2_tex(per_seed)\n",
    "    t3_tex = render_table3_tex(per_seed)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Table 4: Ranking Metrics\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    df_rank = gather_ranking_metrics(exp_dir, ks=(1, 3, 5, 10))\n",
    "    \n",
    "    if df_rank.empty:\n",
    "        print(\"[WARN] No ranking metrics found; Table 4 will be a placeholder.\")\n",
    "        t4_tex = \"% Ranking table unavailable\"\n",
    "    else:\n",
    "        t4_tex = render_table4_tex(df_rank, ks=(1, 3, 5, 10))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Table 5: Fairness (via \\input)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    fairness_tex = find_fairness_tex(exp_dir)\n",
    "    \n",
    "    if fairness_tex is None:\n",
    "        print(\"[WARN] fairness_by_group.tex not found; Table 5 will be a placeholder.\")\n",
    "        t5_tex = \"% fairness_by_group.tex not found\"\n",
    "    else:\n",
    "        rel_fair = os.path.relpath(fairness_tex, start='.')\n",
    "        t5_tex = wrap_input_table(\n",
    "            \"Performance and calibration by subgroup (diagnostics).\",\n",
    "            \"tab:fairness\",\n",
    "            rel_fair\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Table 6: Robustness (via \\input)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    rob_dir, robust_tex = find_robustness_tex(OUTPUTS_DIR)\n",
    "    \n",
    "    if robust_tex is None:\n",
    "        print(\"[WARN] robustness_table_min.tex not found; Table 6 will be a placeholder.\")\n",
    "        t6_tex = \"% robustness_table_min.tex not found\"\n",
    "    else:\n",
    "        rel_rob = os.path.relpath(robust_tex, start='.')\n",
    "        t6_tex = wrap_input_table(\n",
    "            \"Robustness: leakage ablation and inductive (cold-start) evaluation.\",\n",
    "            \"tab:robust\",\n",
    "            rel_rob\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Combine and write output\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    all_tex = \"\\n\\n\".join([t1_tex, t2_tex, t3_tex, t4_tex, t5_tex, t6_tex])\n",
    "\n",
    "    out_tex = os.path.join(OUTPUTS_DIR, \"paper_tables.tex\")\n",
    "    \n",
    "    with open(out_tex, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(all_tex)\n",
    "\n",
    "    print(\"\\n===== LaTeX tables written to =====\")\n",
    "    print(out_tex)\n",
    "    print(\"\\n===== Preview (first 1500 chars) =====\")\n",
    "    print(all_tex[:1500])\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 9: SCRIPT ENTRY POINT\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sanity check: warn about missing data files\n",
    "    for p in [HOUSEHOLDS, STUDENTS]:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[WARN] Not found: {p} (CWD={os.getcwd()})\")\n",
    "    \n",
    "    if not os.path.exists(STAFF):\n",
    "        print(f\"[INFO] Staff file not found; proceeding without works_at edges.\")\n",
    "    \n",
    "    # Generate all tables\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f056e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using experiment directory: ./outputs\\FULL_20250819_130311\n",
      "[OK] Saved ROC to: ./outputs\\FULL_20250819_130311\\roc_by_seed.pdf\n",
      "[OK] Saved PR to: ./outputs\\FULL_20250819_130311\\pr_by_seed.pdf\n",
      "[OK] Saved reliability to: ./outputs\\FULL_20250819_130311\\reliability_by_seed.pdf\n",
      "[OK] Saved score histogram to: ./outputs\\FULL_20250819_130311\\score_hist_by_seed.pdf\n",
      "[OK] Saved calibration histogram to: ./outputs\\FULL_20250819_130311\\calibration_histogram.pdf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PUBLICATION-QUALITY FIGURE GENERATOR\n",
    "================================================================================\n",
    "\n",
    "PAPER REFERENCE\n",
    "---------------\n",
    "This module supports the paper:\n",
    "\n",
    "    \"Calibrated geo-social link prediction for household–school connectivity\n",
    "     in community resilience\"\n",
    "    Gupta, H.S., Biswas, S., & Nicholson, C.D.\n",
    "    International Journal of Disaster Risk Reduction, Volume 131 (2025)\n",
    "    DOI: https://doi.org/10.1016/j.ijdrr.2025.105872\n",
    "\n",
    "OVERVIEW\n",
    "--------\n",
    "Generates high-quality PDF figures for academic publication from the \n",
    "experimental results. All figures are designed to meet journal submission\n",
    "requirements:\n",
    "\n",
    "    - Vector graphics (PDF) for infinite scalability\n",
    "    - TrueType font embedding for consistent rendering in LaTeX/PDF\n",
    "    - Minimal decoration (no figure titles - captions go in LaTeX)\n",
    "    - Single-column width (5.2 inches) suitable for most journals\n",
    "    - Color-blind friendly default color scheme\n",
    "\n",
    "GENERATED FIGURES\n",
    "-----------------\n",
    "    Figure 1: ROC CURVES BY SEED\n",
    "             Receiver Operating Characteristic curves showing discrimination\n",
    "             ability (TPR vs FPR) for each random seed with AUC values.\n",
    "             \n",
    "    Figure 2: PRECISION-RECALL CURVES BY SEED\n",
    "             More informative than ROC for imbalanced data. Shows the trade-off\n",
    "             between precision and recall with Average Precision values.\n",
    "             \n",
    "    Figure 3: RELIABILITY DIAGRAMS BY SEED\n",
    "             Calibration curves comparing predicted probability (confidence)\n",
    "             to empirical accuracy. Perfect calibration lies on the diagonal.\n",
    "             \n",
    "    Figure 4: SCORE DISTRIBUTIONS\n",
    "             Histograms showing separation between positive (actual link)\n",
    "             and negative (no link) predicted probabilities.\n",
    "             \n",
    "    Figure 5: CALIBRATION HISTOGRAM (optional)\n",
    "             Distribution of all predicted probabilities to visualize\n",
    "             model confidence patterns.\n",
    "\n",
    "FIGURE SPECIFICATIONS\n",
    "---------------------\n",
    "    - Size: 5.2 × 4.0 inches (single column)\n",
    "    - Font: 11pt base, 10pt legends/ticks\n",
    "    - Format: PDF with embedded fonts\n",
    "    - Grid: Subtle (alpha=0.25)\n",
    "\n",
    "Authors: Himadri Sen Gupta, Saptadeep Biswas, Charles D. Nicholson\n",
    "Version: 1.0.0\n",
    "License: MIT\n",
    "\n",
    "Usage:\n",
    "    # From Python/Jupyter:\n",
    "    >>> main()  # Generates all figures as PDFs in experiment directory\n",
    "    \n",
    "    # Include in LaTeX:\n",
    "    # \\\\includegraphics[width=\\\\columnwidth]{outputs/FULL_.../roc_by_seed.pdf}\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 1: IMPORTS AND CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Standard library imports\n",
    "import os      # Operating system interface for file/path operations\n",
    "import glob    # Unix-style pathname pattern expansion\n",
    "\n",
    "# Numerical computing and data manipulation\n",
    "import numpy as np    # Numerical arrays and mathematical operations\n",
    "import pandas as pd   # DataFrames for tabular data processing\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib as mpl        # Matplotlib configuration\n",
    "import matplotlib.pyplot as plt # Plotting interface\n",
    "\n",
    "# Evaluation metrics from scikit-learn\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,               # ROC curve computation\n",
    "    auc,                     # Area under curve\n",
    "    precision_recall_curve,  # PR curve computation\n",
    "    average_precision_score  # Average precision\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 2: CONFIGURATION CONSTANTS\n",
    "# ==============================================================================\n",
    "\n",
    "# Output directory configuration\n",
    "OUTPUTS_ROOT = \"./outputs\"  # Root directory containing FULL_* experiments\n",
    "\n",
    "# Figure file names\n",
    "FIG_FILENAMES = {\n",
    "    \"roc\": \"roc_by_seed.pdf\",               # ROC curves\n",
    "    \"pr\": \"pr_by_seed.pdf\",                 # Precision-Recall curves\n",
    "    \"reliability\": \"reliability_by_seed.pdf\", # Reliability diagrams\n",
    "    \"score_hist\": \"score_hist_by_seed.pdf\", # Score distributions\n",
    "    \"calib_hist\": \"calibration_histogram.pdf\",  # Optional calibration histogram\n",
    "}\n",
    "\n",
    "# Evaluation parameters\n",
    "ECE_BINS = 15    # Number of bins for reliability diagrams\n",
    "SCORE_BINS = 50  # Number of bins for score histograms\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 3: MATPLOTLIB CONFIGURATION FOR PUBLICATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Configure matplotlib for high-quality PDF output\n",
    "# TrueType fonts embed properly in LaTeX documents\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42       # TrueType fonts in PDF\n",
    "mpl.rcParams[\"ps.fonttype\"] = 42        # TrueType fonts in PostScript\n",
    "\n",
    "# Font sizes optimized for journal figures\n",
    "mpl.rcParams[\"font.size\"] = 11          # Base font size\n",
    "mpl.rcParams[\"axes.labelsize\"] = 11     # Axis label size\n",
    "mpl.rcParams[\"legend.fontsize\"] = 10    # Legend text size\n",
    "mpl.rcParams[\"xtick.labelsize\"] = 10    # X-axis tick labels\n",
    "mpl.rcParams[\"ytick.labelsize\"] = 10    # Y-axis tick labels\n",
    "\n",
    "# Save figure settings\n",
    "mpl.rcParams[\"savefig.bbox\"] = \"tight\"  # Tight bounding box\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 4: UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def find_latest_dir(root: str, prefix: str) -> str:\n",
    "    \"\"\"\n",
    "    Find the most recently modified directory matching a prefix.\n",
    "    \n",
    "    Searches for directories matching the pattern {root}/{prefix}_* and\n",
    "    returns the most recently modified one based on filesystem timestamp.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "        Root directory to search in.\n",
    "        \n",
    "    prefix : str\n",
    "        Directory name prefix to match (e.g., \"FULL\", \"ROBUST\").\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        Path to the most recent matching directory, or None if not found.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> find_latest_dir(\"./outputs\", \"FULL\")\n",
    "    './outputs/FULL_20240115_143022'\n",
    "    \"\"\"\n",
    "    # Find all directories matching the pattern\n",
    "    cand = [p for p in glob.glob(os.path.join(root, f\"{prefix}_*\")) if os.path.isdir(p)]\n",
    "    \n",
    "    # Return None if no matches found\n",
    "    if not cand:\n",
    "        return None\n",
    "    \n",
    "    # Sort by modification time, most recent first\n",
    "    cand.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    \n",
    "    return cand[0]\n",
    "\n",
    "\n",
    "def load_seed_preds(exp_dir: str) -> list:\n",
    "    \"\"\"\n",
    "    Load per-seed predictions from test_edge_scores.csv files.\n",
    "    \n",
    "    Searches for seed_* directories and loads the test predictions\n",
    "    from each one. Returns a list of dictionaries containing the\n",
    "    seed name, true labels, and predicted probabilities.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    exp_dir : str\n",
    "        Experiment directory containing seed_* subdirectories.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        List of dictionaries with keys:\n",
    "        - 'seed': Seed directory name (str)\n",
    "        - 'y': True binary labels (np.ndarray)\n",
    "        - 'p': Predicted probabilities (np.ndarray)\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> preds = load_seed_preds('./outputs/FULL_20240115')\n",
    "    >>> len(preds)\n",
    "    5\n",
    "    >>> preds[0]['y'].shape\n",
    "    (10000,)\n",
    "    \"\"\"\n",
    "    # Find all seed directories\n",
    "    seeds = sorted([\n",
    "        d for d in glob.glob(os.path.join(exp_dir, \"seed_*\")) \n",
    "        if os.path.isdir(d)\n",
    "    ])\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for sd in seeds:\n",
    "        csv_path = os.path.join(sd, \"test_edge_scores.csv\")\n",
    "        \n",
    "        # Skip if file doesn't exist\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"[WARN] Missing {csv_path}; skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Load predictions\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Validate required columns\n",
    "        if not {\"label\", \"prob\"}.issubset(df.columns):\n",
    "            print(f\"[WARN] {csv_path} lacks needed columns; skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract labels and probabilities\n",
    "        y = df[\"label\"].to_numpy().astype(int)\n",
    "        p = df[\"prob\"].to_numpy().astype(float)\n",
    "        \n",
    "        out.append({\n",
    "            \"seed\": os.path.basename(sd),\n",
    "            \"y\": y,\n",
    "            \"p\": p\n",
    "        })\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def reliability_points(\n",
    "    probs: np.ndarray, \n",
    "    labels: np.ndarray, \n",
    "    n_bins: int = 15\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Compute reliability diagram points for calibration visualization.\n",
    "    \n",
    "    Bins predictions by confidence and computes the average confidence\n",
    "    and empirical accuracy in each bin. A perfectly calibrated model\n",
    "    will have average confidence equal to empirical accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    probs : np.ndarray\n",
    "        Predicted probabilities in [0, 1].\n",
    "        \n",
    "    labels : np.ndarray\n",
    "        Binary ground truth labels (0 or 1).\n",
    "        \n",
    "    n_bins : int, optional (default=15)\n",
    "        Number of equal-width bins.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (bin_centers, avg_confidence, avg_accuracy)\n",
    "        - bin_centers: Midpoints of each bin (np.ndarray)\n",
    "        - avg_confidence: Mean predicted probability per bin (np.ndarray)\n",
    "        - avg_accuracy: Empirical accuracy per bin (np.ndarray)\n",
    "        NaN values indicate empty bins.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> centers, conf, acc = reliability_points(probs, labels, n_bins=10)\n",
    "    >>> # Plot: plt.plot(conf, acc)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays\n",
    "    probs = np.asarray(probs, dtype=float)\n",
    "    labels = np.asarray(labels, dtype=float)\n",
    "    \n",
    "    # Create bins and compute centers\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    \n",
    "    # Assign predictions to bins\n",
    "    inds = np.digitize(probs, bins) - 1\n",
    "    \n",
    "    # Compute per-bin statistics\n",
    "    avg_conf, avg_acc = [], []\n",
    "    \n",
    "    for b in range(n_bins):\n",
    "        mask = inds == b\n",
    "        \n",
    "        if np.any(mask):\n",
    "            # Average confidence and accuracy in this bin\n",
    "            avg_conf.append(float(probs[mask].mean()))\n",
    "            avg_acc.append(float(labels[mask].mean()))\n",
    "        else:\n",
    "            # Empty bin\n",
    "            avg_conf.append(np.nan)\n",
    "            avg_acc.append(np.nan)\n",
    "    \n",
    "    return centers, np.array(avg_conf), np.array(avg_acc)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 5: FIGURE GENERATION FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_roc_by_seed(seed_data: list, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate ROC curves for each seed, overlaid on single plot.\n",
    "    \n",
    "    Creates a publication-quality figure showing ROC curves for all\n",
    "    experimental seeds. Includes the diagonal reference line (chance).\n",
    "    No figure title is included (for journal submission).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_data : list\n",
    "        List of prediction dictionaries from load_seed_preds().\n",
    "        \n",
    "    save_path : str\n",
    "        Output file path for the PDF figure.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Saves figure to save_path.\n",
    "    \"\"\"\n",
    "    # Create figure with appropriate size for single-column journal\n",
    "    fig, ax = plt.subplots(figsize=(5.2, 4.0))\n",
    "    \n",
    "    # Plot ROC curve for each seed\n",
    "    for item in seed_data:\n",
    "        # Compute ROC curve points\n",
    "        fpr, tpr, _ = roc_curve(item[\"y\"], item[\"p\"])\n",
    "        \n",
    "        # Compute AUC for legend\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot with seed label and AUC\n",
    "        ax.plot(fpr, tpr, lw=1.5, label=f\"{item['seed']} (AUC={roc_auc:.3f})\")\n",
    "    \n",
    "    # Add diagonal reference line (random classifier)\n",
    "    ax.plot([0, 1], [0, 1], lw=1.0, ls=\"--\", color=\"0.5\", label=\"chance\")\n",
    "    \n",
    "    # Configure axes\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    \n",
    "    # Add legend without frame\n",
    "    ax.legend(frameon=False, loc=\"lower right\", ncol=1)\n",
    "    \n",
    "    # Add subtle grid\n",
    "    ax.grid(alpha=0.25, linewidth=0.5)\n",
    "    \n",
    "    # Save and close\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_pr_by_seed(seed_data: list, save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate Precision-Recall curves for each seed, overlaid on single plot.\n",
    "    \n",
    "    Creates a publication-quality figure showing PR curves for all\n",
    "    experimental seeds. PR curves are more informative than ROC for\n",
    "    imbalanced datasets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_data : list\n",
    "        List of prediction dictionaries from load_seed_preds().\n",
    "        \n",
    "    save_path : str\n",
    "        Output file path for the PDF figure.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Saves figure to save_path.\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(5.2, 4.0))\n",
    "    \n",
    "    # Plot PR curve for each seed\n",
    "    for item in seed_data:\n",
    "        # Compute PR curve points\n",
    "        precision, recall, _ = precision_recall_curve(item[\"y\"], item[\"p\"])\n",
    "        \n",
    "        # Compute average precision for legend\n",
    "        ap = average_precision_score(item[\"y\"], item[\"p\"])\n",
    "        \n",
    "        # Plot with seed label and AP\n",
    "        ax.plot(recall, precision, lw=1.5, label=f\"{item['seed']} (AP={ap:.3f})\")\n",
    "    \n",
    "    # Configure axes\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(frameon=False, loc=\"lower left\", ncol=1)\n",
    "    \n",
    "    # Add subtle grid\n",
    "    ax.grid(alpha=0.25, linewidth=0.5)\n",
    "    \n",
    "    # Save and close\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_reliability_by_seed(\n",
    "    seed_data: list, \n",
    "    save_path: str, \n",
    "    n_bins: int = ECE_BINS\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate reliability diagrams for each seed (calibration curves).\n",
    "    \n",
    "    Creates a publication-quality figure showing calibration curves.\n",
    "    Each curve shows the relationship between predicted probability\n",
    "    (confidence) and empirical accuracy. Perfect calibration lies\n",
    "    on the diagonal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_data : list\n",
    "        List of prediction dictionaries from load_seed_preds().\n",
    "        \n",
    "    save_path : str\n",
    "        Output file path for the PDF figure.\n",
    "        \n",
    "    n_bins : int, optional (default=ECE_BINS)\n",
    "        Number of calibration bins.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Saves figure to save_path.\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(5.2, 4.0))\n",
    "    \n",
    "    # Plot reliability curve for each seed\n",
    "    for item in seed_data:\n",
    "        # Compute reliability points\n",
    "        centers, conf, acc = reliability_points(item[\"p\"], item[\"y\"], n_bins=n_bins)\n",
    "        \n",
    "        # Mask NaN values for clean plotting\n",
    "        m = ~np.isnan(conf) & ~np.isnan(acc)\n",
    "        \n",
    "        # Plot with markers\n",
    "        ax.plot(conf[m], acc[m], lw=1.5, marker=\"o\", ms=3, label=f\"{item['seed']}\")\n",
    "    \n",
    "    # Add diagonal reference line (perfect calibration)\n",
    "    ax.plot([0, 1], [0, 1], lw=1.0, ls=\"--\", color=\"0.5\", label=\"perfect\")\n",
    "    \n",
    "    # Configure axes\n",
    "    ax.set_xlabel(\"Mean Predicted Probability\")\n",
    "    ax.set_ylabel(\"Empirical Accuracy\")\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(frameon=False, loc=\"lower right\", ncol=1)\n",
    "    \n",
    "    # Add subtle grid\n",
    "    ax.grid(alpha=0.25, linewidth=0.5)\n",
    "    \n",
    "    # Save and close\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_score_histogram(\n",
    "    seed_data: list, \n",
    "    save_path: str, \n",
    "    bins: int = SCORE_BINS\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate score distribution histograms for positive and negative samples.\n",
    "    \n",
    "    Creates a publication-quality figure showing the distribution of\n",
    "    predicted probabilities for positive (actual link) vs negative\n",
    "    (no link) samples. Uses step histograms to avoid occlusion.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_data : list\n",
    "        List of prediction dictionaries from load_seed_preds().\n",
    "        \n",
    "    save_path : str\n",
    "        Output file path for the PDF figure.\n",
    "        \n",
    "    bins : int, optional (default=SCORE_BINS)\n",
    "        Number of histogram bins.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Saves figure to save_path.\n",
    "    \"\"\"\n",
    "    # Aggregate scores across all seeds\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    \n",
    "    for item in seed_data:\n",
    "        y, p = item[\"y\"], item[\"p\"]\n",
    "        pos_scores.append(p[y == 1])  # Positive samples\n",
    "        neg_scores.append(p[y == 0])  # Negative samples\n",
    "    \n",
    "    # Concatenate all seeds\n",
    "    pos_scores = np.concatenate(pos_scores) if pos_scores else np.array([])\n",
    "    neg_scores = np.concatenate(neg_scores) if neg_scores else np.array([])\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(5.2, 4.0))\n",
    "    \n",
    "    # Plot histograms using step style (no fill) to avoid occlusion\n",
    "    if pos_scores.size > 0:\n",
    "        ax.hist(\n",
    "            pos_scores, \n",
    "            bins=np.linspace(0, 1, bins + 1), \n",
    "            histtype=\"step\", \n",
    "            lw=1.5, \n",
    "            label=\"Positive\", \n",
    "            density=True\n",
    "        )\n",
    "    \n",
    "    if neg_scores.size > 0:\n",
    "        ax.hist(\n",
    "            neg_scores, \n",
    "            bins=np.linspace(0, 1, bins + 1), \n",
    "            histtype=\"step\", \n",
    "            lw=1.5, \n",
    "            label=\"Negative\", \n",
    "            density=True\n",
    "        )\n",
    "\n",
    "    # Configure axes\n",
    "    ax.set_xlabel(\"Predicted Probability\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend(frameon=False, loc=\"upper center\", ncol=2)\n",
    "    \n",
    "    # Add subtle grid\n",
    "    ax.grid(alpha=0.25, linewidth=0.5)\n",
    "    \n",
    "    # Save and close\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_calibration_histogram(\n",
    "    seed_data: list, \n",
    "    save_path: str, \n",
    "    bins: int = SCORE_BINS\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate histogram of all predicted probabilities (optional Figure 5).\n",
    "    \n",
    "    Creates a simple histogram showing the distribution of predicted\n",
    "    probabilities across all samples and seeds. Useful for understanding\n",
    "    model confidence patterns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed_data : list\n",
    "        List of prediction dictionaries from load_seed_preds().\n",
    "        \n",
    "    save_path : str\n",
    "        Output file path for the PDF figure.\n",
    "        \n",
    "    bins : int, optional (default=SCORE_BINS)\n",
    "        Number of histogram bins.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Saves figure to save_path.\n",
    "    \"\"\"\n",
    "    # Aggregate all probabilities\n",
    "    all_probs = []\n",
    "    \n",
    "    for item in seed_data:\n",
    "        all_probs.append(item[\"p\"])\n",
    "    \n",
    "    all_probs = np.concatenate(all_probs) if all_probs else np.array([])\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(5.2, 4.0))\n",
    "    \n",
    "    # Plot histogram with bars\n",
    "    if all_probs.size > 0:\n",
    "        ax.hist(\n",
    "            all_probs, \n",
    "            bins=np.linspace(0, 1, bins + 1), \n",
    "            histtype=\"bar\", \n",
    "            lw=0.8, \n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "    \n",
    "    # Configure axes\n",
    "    ax.set_xlabel(\"Predicted Probability\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    \n",
    "    # Add subtle grid\n",
    "    ax.grid(alpha=0.25, linewidth=0.5)\n",
    "    \n",
    "    # Save and close\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 6: MAIN ORCHESTRATION FUNCTION\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to generate all publication figures.\n",
    "    \n",
    "    Orchestrates the generation of:\n",
    "    - Figure 1: ROC curves by seed\n",
    "    - Figure 2: PR curves by seed\n",
    "    - Figure 3: Reliability diagrams by seed\n",
    "    - Figure 4: Score distributions\n",
    "    - Figure 5: Calibration histogram (optional)\n",
    "    \n",
    "    All figures are saved as PDF files in the experiment directory.\n",
    "    \"\"\"\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Find experiment directory\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    exp_dir = find_latest_dir(OUTPUTS_ROOT, \"FULL\")\n",
    "    \n",
    "    if exp_dir is None:\n",
    "        raise RuntimeError(f\"No FULL_* experiment directory found in {OUTPUTS_ROOT}\")\n",
    "    \n",
    "    print(f\"[INFO] Using experiment directory: {exp_dir}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Load predictions\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    seed_data = load_seed_preds(exp_dir)\n",
    "    \n",
    "    if not seed_data:\n",
    "        raise RuntimeError(\n",
    "            f\"No seed_* folders with test_edge_scores.csv found under {exp_dir}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Figure 1: ROC curves\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    out_roc = os.path.join(exp_dir, FIG_FILENAMES[\"roc\"])\n",
    "    plot_roc_by_seed(seed_data, out_roc)\n",
    "    print(f\"[OK] Saved ROC to: {out_roc}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Figure 2: PR curves\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    out_pr = os.path.join(exp_dir, FIG_FILENAMES[\"pr\"])\n",
    "    plot_pr_by_seed(seed_data, out_pr)\n",
    "    print(f\"[OK] Saved PR to: {out_pr}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Figure 3: Reliability diagrams\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    out_rel = os.path.join(exp_dir, FIG_FILENAMES[\"reliability\"])\n",
    "    plot_reliability_by_seed(seed_data, out_rel, n_bins=ECE_BINS)\n",
    "    print(f\"[OK] Saved reliability to: {out_rel}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Figure 4: Score distributions\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    out_hist = os.path.join(exp_dir, FIG_FILENAMES[\"score_hist\"])\n",
    "    plot_score_histogram(seed_data, out_hist, bins=SCORE_BINS)\n",
    "    print(f\"[OK] Saved score histogram to: {out_hist}\")\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Generate Figure 5: Calibration histogram (optional)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    out_ch = os.path.join(exp_dir, FIG_FILENAMES[\"calib_hist\"])\n",
    "    plot_calibration_histogram(seed_data, out_ch, bins=SCORE_BINS)\n",
    "    print(f\"[OK] Saved calibration histogram to: {out_ch}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECTION 7: SCRIPT ENTRY POINT\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba49026",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Execution Guide\n",
    "\n",
    "### Step 1: Run the Main Training Pipeline (Cell 2)\n",
    "\n",
    "The main code cell contains the complete GNN pipeline. To reproduce results:\n",
    "\n",
    "```python\n",
    "# Option A: Run robustness suite (Standard + Cold-start experiments)\n",
    "paths = {\n",
    "    'households': 'hui_v0-1-0_Lumberton_NC_2010_rs9876.csv',\n",
    "    'students': 'prec_v0-2-0_Lumberton_NC_2010_rs9876_students.csv',\n",
    "    'staff': 'prec_v0-2-0_Lumberton_NC_2010_rs9876_schoolstaff.csv'\n",
    "}\n",
    "run_robustness_suite(paths, device='cuda', seed=42)\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "- `outputs/ROBUST_YYYYMMDD_HHMMSS/A_standard/` - Standard holdout results\n",
    "- `outputs/ROBUST_YYYYMMDD_HHMMSS/B_coldstart_households/` - Cold-start households\n",
    "- `outputs/ROBUST_YYYYMMDD_HHMMSS/C_coldstart_schools/` - Cold-start schools\n",
    "\n",
    "### Step 2: Generate LaTeX Tables (Cell 3)\n",
    "\n",
    "After training completes, run cell 3 to generate publication-ready tables:\n",
    "\n",
    "```python\n",
    "main()  # Generates paper_tables.tex\n",
    "```\n",
    "\n",
    "### Step 3: Generate Publication Figures (Cell 4)\n",
    "\n",
    "Run cell 4 to generate PDF figures:\n",
    "\n",
    "```python\n",
    "main()  # Generates roc_by_seed.pdf, pr_by_seed.pdf, etc.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| `ModuleNotFoundError: torch_cluster` | Install via `pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html` |\n",
    "| `FileNotFoundError: CSV files` | Update paths dictionary to point to your data location |\n",
    "| `CUDA out of memory` | Reduce `FT_BATCH` in `DEFAULT_CFG` or use `device='cpu'` |\n",
    "| `No FULL_* directory found` | Run the training pipeline (Cell 2) first |\n",
    "\n",
    "---\n",
    "\n",
    "## Reproducing Paper Results\n",
    "\n",
    "To exactly reproduce the results reported in the paper:\n",
    "\n",
    "1. **Use the same data**: Lumberton, NC 2010 synthetic population (rs9876)\n",
    "2. **Use the same seeds**: `seed=42` (primary), `seed=123, 456, 789, 1000` (for variance)\n",
    "3. **Use default hyperparameters**: The `DEFAULT_CFG` dictionary contains tuned values\n",
    "4. **Run all three experiments**: Standard, Cold-start Households, Cold-start Schools\n",
    "\n",
    "The paper reports mean ± std across 5 random seeds for each metric.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
